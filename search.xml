<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小萌是猪！]]></title>
    <url>%2Fmm-is-a-pig-md%2F</url>
    <content type="text"><![CDATA[嘿嘿!]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu环境下pyrouge+ROUGE安装]]></title>
    <url>%2FUbuntu%E7%8E%AF%E5%A2%83%E4%B8%8Bpyrouge-ROUGE%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[记录Ubuntu环境下ROUGE+pyrouge的配置过程。 1. ROUGE1.1 下载安装1.1.1 下载依赖12sudo cpan install DB_filesudo cpan install XML::DOM 1.1.2 下载ROUGE下载ROUGE-1.5.5: 链接: https://pan.baidu.com/s/1OhC1NmQcLEMoNGTnksVaXQ 提取码: uyk6解压到~/ROUGE，添加环境变量123vim ~/.profileexport ROUGE_EVAL_HOME="~/ROUGE/data"source ~/.profile 1.1.3 测试12cd ~/ROUGE./runROUGE-test.pl 出现以下信息表示安装成功：123456789101112131415../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-a.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-a-m.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m -s ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-a-m-s.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -l 10 -a ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-l10-a.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -l 10 -a -m ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-l10-a-m.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -l 10 -a -m -s ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-l10-a-m-s.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -b 75 -a ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-b75-a.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -b 75 -a -m ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-b75-a-m.out../ROUGE-1.5.5.pl -e ../data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -b 75 -a -m -s ROUGE-test.xml &gt; ../sample-output/ROUGE-test-c95-2-1-U-r1000-n4-w1.2-b75-a-m-s.out../ROUGE-1.5.5.pl -e ../data -3 HM -z SIMPLE DUC2002-BE-F.in.26.lst 26 &gt; ../sample-output/DUC2002-BE-F.in.26.lst.out../ROUGE-1.5.5.pl -e ../data -3 HM DUC2002-BE-F.in.26.simple.xml 26 &gt; ../sample-output/DUC2002-BE-F.in.26.simple.out../ROUGE-1.5.5.pl -e ../data -3 HM -z SIMPLE DUC2002-BE-L.in.26.lst 26 &gt; ../sample-output/DUC2002-BE-L.in.26.lst.out../ROUGE-1.5.5.pl -e ../data -3 HM DUC2002-BE-L.in.26.simple.xml 26 &gt; ../sample-output/DUC2002-BE-L.in.26.simple.out../ROUGE-1.5.5.pl -e ../data -n 4 -z SPL DUC2002-ROUGE.in.26.spl.lst 26 &gt; ../sample-output/DUC2002-ROUGE.in.26.spl.lst.out../ROUGE-1.5.5.pl -e ../data -n 4 DUC2002-ROUGE.in.26.spl.xml 26 &gt; ../sample-output/DUC2002-ROUGE.in.26.spl.out 2. pyrougepyrouge要安装最新版，否则测试时会出现error和failure。123git clone https://github.com/bheinzerling/pyrougecd pyrougepython setup.py install 指定之前ROUGE位置：1pyrouge_set_rouge_path ~/ROUGE 测试：1python -m pyrough.test 出现ok即安装成功。 参考资料 WORKING WITH ROUGE 1.5.5 EVALUATION METRIC IN PYTHON Unittests fail]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>配置</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu装机记录]]></title>
    <url>%2Fubuntu%2F</url>
    <content type="text"><![CDATA[前两天不慎摔坏了笔记本硬盘，换了一块新的ssd，需要重新装系统软件及进行相关配置，在此做一个记录。 环境：Ubuntu16.04 装系统就不具体展开了，注意的是我装了英文版，因为如果装中文版的话，用户目录下的文件夹会初始化成中文名，终端中敲中文有点麻烦，需要后期更改。因此就直接装英文版了～ 更换源由于墙的原因，Ubuntu官方源不怎么好使，先换成阿里云的源。1sudo vi /etc/apt/sources.list 1234567891011deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiverse deb http://archive.canonical.com/ubuntu/ xenial partner 1sudo apt-get update 搜狗输入法第一件事肯定是先装个中文输入法，方便后续工作。Ubuntu下搜狗输入法是一个比较好的选择（可能也是唯一？）。先在官网下deb包，链接。cd到deb包所在的目录，安装：12sudo apt-get install -fsudo dpkg -i sogou*.deb 设置系统默认输入法由iBus改为fcitx：system setting–&gt;language support–&gt;下拉iBus改为fcitx注销，重新登录。右键右上角键盘图标，进入设置，点左下角加号，添加sogou输入法。 ps: 如果出现两个搜狗拼音的图标，解决方法1sudo apt-get remove fcitx-ui-qimpanel ssh这个比较简单1sudo apt-get install openssh-server 装好默认会启动，如果没有启动可以按以下命令启动1sudo /etc/init.d/ssh start 查看是否启动1ps -e | grep ssh 如果能看到sshd 就说明已经启动。 ChromeUbuntu自带的是Firefox，而我习惯了Chrome，因此Chrome是必备的。首先添加google软件的源跟验证公钥，并更新源。123sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -sudo apt-get update 然后安装：1sudo apt-get install google-chrome-stable 安装成功后可以通过以下命令启动：1/usr/bin/google-chrome-stable 将其锁定到launcher就可以单击启动了，可以跟FireFox说byebye了～ Shadowsocks由于众所周知的原因，ss也是必须的。 安装ss123sudo apt-get install python-pipsudo apt-get install python-setuptools m2cryptosudo pip install shadowsocks 如果ss加密方式使用的是chacha-20-ietf-poly1305，由于以上安装方式并不支持该加密方式，需要安装libsodium-dev：12sudo apt install libsodium-devsudo pip install https://github.com/shadowsocks/shadowsocks/archive/master.zip -U 可以参考这里。 ps: 以上安装shadowsocks的两条命令都用了sudo，是为了方便后续配置自动启动，如果不安装在root下，后续配置会比较麻烦。 启动：推荐使用配置文件的方式启动，一劳永逸，配置文件shadowsocks.json如下：12345678&#123; "server": "", //服务器ip "server_port": , //端口 "local_port": 1080, //本地端口 "password": "", //密码 "timeout": 600, "method": "chacha-20-ietf-poly1305" //加密方式&#125; 1sudo sslocal -c ~/Documents/shadowsocks.json 设置开机自动启动1sudo vim /etc/rc.local 在exit 0前添加启动命令：1sudo sslocal -c /home/**/Documents/shadowsocks.json &amp; ps: 这里最后的&amp;不要省略，去掉&amp;的话启动时会等这条命令执行完再去执行后面的命令，如果这里卡住了就开不了机了。一个坑是装了Gnome后开机会一直转圈 配置浏览器以上只是安装了ss，还需要在浏览器中进行配置，才可以科学上网。首先要安装SwitchOmega插件，下载地址。下载完后拖到Chrome插件页面chrome://extensions/就行了（如果拖曳安装失败，打开插件页面的开发者模式，重启Chrome即可）。 进入SwitchOmega配置页面，新建情景模式，名字随便取，选择代理服务器（Proxy Profile）。 代理协议选择SOCKS5，代理服务器127.0.0.1，端口1080，保存。 设置自动切换，让不需要翻墙的流量走直连，选左边的自动切换，规则列表网址中填https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt， 立即更新情景模式，可以看到一大批无法访问的网站会被添加到规则列表里。在切换规则中选择刚才的情景，保存即可。 配置全局ss通过以上配置，可以在浏览器中科学上网，但有时候要在终端中科学上网，比如git如果不用科学上网的话，push和pull会很慢。 安装privoxy：1sudo apt install privoxy 配置privoxy：1sudo vim /etc/privoxy/config 去掉1336行注释，监听端口1080，注意最后有个点。1forward-socks5t / 127.0.0.1:1080 . 监听默认8118端口1listen-address localhost:8118 设置http和https的全局代理，在环境变量中加入下面两行12export http_proxy=http://localhost:8118export https_proxy=https://localhost:8118 应用生效1source /etc/profile 启动服务1sudo service privoxy start 可以通过curl来测试是否成功开启全局ss：1curl www.google.com 如果开启成功，会返回google页面，失败的话没反应。 ps: 之后终端中默认是科学上网的，如果要下载大容量的文件，例如百度云上的，为了节约流量，同时国内网站科学上网反而为降低速度，注释掉配置文件/etc/privoxy/config中的1336行即可 GnomeGnome安装配置比较简单，但是不推荐使用，存在一些问题，theme也没有unity桌面的好看。1sudo apt-get install ubuntu-gnome-desktop 会出现一个图形界面，两个选项选任意一个都行，装完后重启，登录时选择Gnome桌面即可。原来的桌面可以删除：1sudo apt-get remove ubuntu-desktop ps: 这里的一个坑是，我装了Gnome后，之前开机自启的sslocal进程变得无效，即进程正常启动了，但是不能科学上网。解决办法：试了很多种都没用，最后只能创建一个sslocal的快捷方式，然后把它放到Gnome的开机自启中去。 创建启动脚本1sudo vim /usr/local/bin/autostart_sslocal 12#! /bin/bash/usr/local/bin/sslocal -c /home/**/Documents/shadowsocks.json 创建快捷方式1sudo vim /usr/share/applications/sslocal.desktop 12345678910[Desktop Entry]Encoding=UTF-8Version=1.0Name=sslocalGenericName=sslocalComment=sslocalExec=/usr/local/bin/autostart_sslocalTerminal=falseType=ApplicationX-Desktop-File-Install-Version=0.22 加入启动项1ln -s /usr/share/applications/sslocal.desktop ~/.config/autostart/sslocal.desktop Gnome-theme启动tweak-tool，在扩展中打开user-theme选项即可。 我使用的配置：主题： arc扩展：Dash to DockHide Top BarUser Themes 以上是Gnome主题的配置，Unity的配置为：主题123sudo add-apt-repository ppa:noobslab/themes sudo apt-get update sudo apt-get install flatabulous-theme 图标123sudo add-apt-repository ppa:noobslab/icons sudo apt-get update sudo apt-get install ultra-flat-icons 字体maonaco-font-master.zip解压后cd到目录中安装：1sudo ./install-font-ubuntu.sh https://github.com/todylu/monaco.ttf/blob/master/monaco.ttf?raw=true 安装完后用unity-tweak-tool工具替换。 zsh安装安装zsh1sudo apt-get install zsh 安装oh-my-zsh1sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)" 安装完重启后打开终端即自动进入zsh。 配置主题：ys123vim ~/.zshrcset ZSH_THEME="ys"source ~/.zshrc 扩展:终端高亮1234git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlightingvim ~/.zshrcset plugins=([plugins] zsh-syntax-highlighting)source ~/.zshrc 自动补全可能路径 1234git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestionsvim ~/.zshrcset plugins=([plugins] zsh-autosuggestions)source ~/.zshrc vim配置文件1vim --version 如果clipboard选项是关闭的（-），安装vim-gtk。之后可以用”+y/p跟外部进行复制粘贴。 Tensorflow1pip install --ignore-installed --upgrade url Python 2.7 CPU only:https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl GPU support:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.1-cp27-none-linux_x86_64.whl Python 3.4 CPU only:https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp34-cp34m-linux_x86_64.whl GPU support:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.1-cp34-cp34m-linux_x86_64.whl Python 3.5 CPU only:https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp35-cp35m-linux_x86_64.whl GPU support:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.1-cp35-cp35m-linux_x86_64.whl Python 3.6 CPU only:https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl GPU support:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.1-cp36-cp36m-linux_x86_64.whl]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>配置</tag>
        <tag>装机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法面试部分题目整理]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[记录下2018秋招算法岗的一些面经，以及整理的部分题目。 机器学习理论1. XGBoost和GradientBoost的区别是什么？XGBoost的基分类器只能是树吗？如果基分类器是线性分类器会是怎么样？机器学习问答1. AdaBoost和GBDT的主要相同点和区别是什么？相同点： 都是加性模型 每一步训练一个弱学习器（通常是决策树）以弥补前面模型的不足 不同点： AdaBoost中当前学习器的“不足”由样本权重来决定 GBDT中当前学习器的“不足”由求梯度决定 GBDT是Adaboost的推广，GBDT可以使用不同的损失函数 2. 为什么我们不能用最小二乘法来解决二元分类问题？换句话说，我们为什么不能用平方误差(MSE)作为逻辑回归的损失函数？从下面几个方面，我们都可以看出MSE是不适合作为逻辑回归的损失函数的： MSE不是accuracy的良性代理函数，具体看这里。 最大似然估计的角度来说，logloss是最适合二元分类的损失函数 如果用MSE作为逻辑回归的损失函数，那么这个损失函数本身不是凸函数，而是有很多的局部最优 如果用MSE作为逻辑回归的损失函数，容易发生梯度消失 3. 如何从流数据中均匀取样？具体来说，数据样本是一个个进入系统，你并不知道数据流何时结束或者数据总个数，在这种情况下，你如何一个流动的数据里随机均匀地选出k个样本？如何证明你的方法是随机均匀的？这个问题的解法是很著名的蓄水池采样法(reservoir sampling)。 一般解决方法 先把数据数一遍，得到个数之后，再开始等概率抽取。蓄水池抽样算法 当数据一个个进入系统时，先保留全部前$k$个数据，然后开始读取第$k+1$数据，我们以$1/(k+1)$的概率略过该数据，以$k/(k+1)$的概率保留这个数据，并且从之前的$k$个数据中随机挑选一个丢弃。我们继续读取下一个数据，我们以$k/(k+2)$的概率保留这个数据，并且从之前保留的$k$个数据中随机挑选一个丢弃。归纳起来，对于第$j$个数据，我们以$k/j$的概率保留这个数据，并且从之前保留的$k$个数据中随机挑选一个丢弃。以此规则，反复进行。 相比于一般解决方法，蓄水池的优势显而易见。在时间上，不用不停地遍历整个数据流来随机抽取，更重要的时候，不用保存历史数据，只需要保存$k$个已经被选择的数据。省时间、省空间。蓄水池采样法也是一种为online算法。 等概率性的证明用数学归纳法证明，每个样本被选中的概率的都是相等的。初始情况是，当前只有$k$个样本，此时每个样本都被选中进入池子，也就是$k/k=1$的概率。如果我们一共有$n$个数据点，假设每个数据点被选进入池子的概率相等，都是$k/n$。根据数学归纳法的思想，下面我们只要证明如果一共有$n+1$个数据点，每个数据进入池子中的概率都是$k/(n+1)$。对于第$n+1$个样本$X_{n+1}$，它进入池子的概率显然是$k/(n+1)$。对于第$j$个样本$X_j，j≤n$，它在前一轮中已经在池子里的概率为$k/n$。下面我们分两种情况讨论：第一种情况，$X_{n+1}$没被选中，所以$X_j$依然留在池子中。这种情况的概率为$$P_1=1-\frac{k}{n+1}=\frac{n+1-k}{n+1}$$第二种情况，$X_{n+1}$被选中但是$X_j$没有被替换。这种情况发生的概率为$$P_2=\frac{k}{n+1}\frac{k-1}{k}=\frac{k-1}{n+1}$$两者相加$$P_1+P_2=\frac{n}{n+1}$$所以$X_j$此时在池子中的概率为$$\frac{k}{n}(P_1+P_2)=\frac{k}{n}\frac{n}{n+1}=\frac{k}{n+1}$$证毕。 4. GBDT+LR分类器的原理是什么？GBDT+LR中，如果GBDT有1000颗树，每个树有100个叶子节点，那么输入到LR的特征会是一个高维稀疏的向量。它的维度是多少？那么应该如何处理这个高维度问题？简单来说是将GBDT中每棵树的叶节点作为一个新的特征：每个叶子都对应着一个01变量，如果某样本落在该叶节点上就是1，否则为0。如果有$T$棵树，叶子的总数为$M$，那么每个样本就被转换成一个长度为$M$的01向量，而每个向量都只含有$T$个1。接下来就将这01向量作为逻辑回归模型(LR)的输入。 向量的维度是$1000×100=10^5$。考虑到维度很高，所以我们可以对数据进行降维，比如使用PCA，tSNE或者Autoencoder，然后对降维后的数据进行逻辑回归。 5. 假设你现在有LinkedIn上所有用户的各项数据，你如何预测某个用户这三个月内是否跳槽？你准备使用哪些数据？使用什么模型？首先，很明显这是一个二元分类的问题。1代表三个月内跳槽；0代表三个月内并没有跳槽。因为涉及到跳槽，所以我们也只需要针对目前有工作的用户，排除在读学生和应届生。 既然是监督式学习，就要考虑如何构建training data。一种思路是: 在过去的历史数据中，每三个月的第一天，对用户信息进行快照(snapshot)，并跟踪三个月观测他们是否跳槽。比如说，我们回看过去3年的数据 2015年6月1日，对所有用户进行快照取得训练数据，跟踪用户在2015年6月1日到2015年8月31日内是否跳槽。 2015年9月1日，对所有用户进行快照取得训练数据，跟踪用户在2015年9月1日到2015年11月30日内是否跳槽。 2015年12月1日，对所有用户进行快照取得训练数据，跟踪用户在2015年12月1日到2016年2月29日内是否跳槽。 … 2018年6月1日，对所有用户进行快照取得训练数据，跟踪用户在2018年6月1日到2018年8月30日内是否跳槽。 下面就要具体去考虑需要收集的信息，为训练做准备。要收集的信息可以包括用户基本资料、在snapshot的时刻用户历史数据以及行为以及整个市场的供需状况。 用户资料特征包括：毕业年份、毕业学校、专业、之前的公司、当前的公司、目前的职位、职位的级别、在当前职位的时间长度、在当前公司的时间长度，等等。 Snapshot时的数据以及用户行为。一个是snapshot的年、月、日信息（比如年初跳槽就比较高发）。行为包括：截止到snapshot的时间用户在Linkedin有多少人脉联系人(connections)，最近一周、一个月、两个月、三个月、半年新增的connections个数，新增connections中HR的个数，最近一周、一个月、两个月、三个月、半年访问Linkedin的次数，最近一次更新简历至今的天数、最近一次增加技能至今的天数，最近一周、一个月、两个月、三个月、半年在Linkedin搜索职位的次数，最近一周、一个月、两个月、三个月、半年在Linkedin与HR发消息的次数等等。 整个市场的供需状况。对每个行业/领域/地区，过去每月在Linkedin新发布的职位的个数。 下一个步骤就是建立预测模型。常见二元分类模型包括逻辑回归、随机森林、gbdt/xgboost等。上述提到了很多相关性较强的变量，所以可以使用树类模型或者带正则项的逻辑回归。需要注意的是，这个二元分类显然是非平衡的，1的比例非常少；样本可以通过欠采样或者SMOTE进行预处理，此外也要考虑到模型评价标准的选择。如果最终预测结果为概率，可以使用ROC AUC；如果最终输出结果是0、1标签，可以选择是kappa值。 6. 检测Twitter上的假账号问题: Twitter上有很多假账号，他们影响到很多用户的正常使用。如果你有Twitter上所有用户的信息，以及一些被标记出来的假账号，你怎么利用这些信息来检测出Twitter上的假账号？如果你有Twitter上所有用户的信息，但是这些用户都没有被标记，那么你怎么利用这些信息来检测出Twitter上的假账号？ 这个问题是典型的异常检测(anomaly detection)。根据题意，第一个问题是有监督的异常检测，而第二个问题是无监督的异常检测。 下一步就是根据Twitter的场景来选择特征构造数据集。 用户头像。特征包括：用户是否上传头像、头像的分辨率、近一年更换头像的次数。假账号往往没有头像、或者使用系统默认头像、或者低分辨率头像，而且一旦设定就很少再换了。 用户资料。特征包括：用户是否有自我介绍、用户是否选择了性别、是否选择了地区、是否填写了其他信息、是否绑定了其他社交账号、是否设置个性化的页面短链接。 互粉情况。特征包括：用户follow了多少人、被多少人follow、两者的比例、有多少人与该用户是互粉的。通常假账号会follow很多人，但是却没有很多粉丝。 发推文情况。特征包括：发推文的个数、、发推文的时间段、推文中是否含有地理定位、推文中是否带有超链接。假账号发的推文常带有超链接、却很少带有地理定位，而且发推文的时间段可能是全天时段的。 与他人互动情况。特征包括：用户的推文是否被转载、该用户是否转发过他人的推文、与他人是否有评论、私信的互动。 监督式异常检测下一步就是建立二元分类模型，值得注意的是这里非平衡问题，所以要进行适当的预处理、选择适当的模型以及模型评判标准。在回答建立模型这一部分，请选择自己擅长的模型，因为面试官会根据求职者选择的模型深挖模型细节。 非监督式异常检测非监督式异常检测又分为基于模型和基于统计量的。常见模型包括one-class SVM, Isolation Forests，Autoencoder（Replicator Neural Networks, 教程以及代码实战），DBSCAN，GMM，等。同样，在回答建立模型这一部分，请选择自己擅长的模型，因为面试官会根据求职者选择的模型深挖模型细节。基于统计量的方法通常基于假设检验，其思想是原假设是该用户服从整体用户的分布，对立假设是该用户不来自整体用户的分布，然后对假设检验的p-value设定阈值。 7. 预测沃尔玛门店的某种商品销量准确预测产品的效率有助于仓储、物流的调控。如果你有过去该商品在各个门店每周的销量，你如何建立模型预测未来的销量？除了销量信息，你还想获取哪些有助于模型的信息？如何评判模型的准确度？ 第一个问题是一个一元变量的时间序列问题。这类问题的经典解法是AR，ARMA以及ARIMA和SARIMA模型。此外也可以考虑非线性模型，比如Random Forest或者Boosting，但是在使用这类模型时首先需要de-trend，就是先用线性模型拟合，让整个时间序列平稳，然后再用随机森林或者boosting。此外还有注意合成一些时间变量，比如这周是否是节日等等。因为需要预测多个门店的销量，所以可以选择每个门店一个模型也可以让门店成为模型中的一个特征。此外，也可以尝试RNN等神经网络模型。 这个部分是非常主观的，答案可以各种各样，可供参考的其他有助于模型提高精度的信息包括： 1. 促销打折信息。需要过去该商品的促销打折信息，以及未来的促销打折计划。 2. 其他同类产品过去的销量。 3. 各个门店所在地的消费价格指数CPI。 4. 各个门店所在地的天气状况。 5. 各个门店所在地是否有大型活动、突发事故等。 时间序列无法进行交叉验证，否则会发生data leakage。可行的方法是，用前m个月的数据进行训练，用后n个月进行验证。因为这是一个回归问题，所以评价标准可以使用RMSE，MAE，MAPE等。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>面试</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于朴素贝叶斯的短文本分类]]></title>
    <url>%2Fbayes_tfidf%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯介绍，以及如何将它用在文本分类上。进一步地，如何引入卡方及tf-idf进行特征选择。 朴素贝叶斯贝叶斯定理简介关于贝叶斯定理的文章网上已经有很多了，这里只做简单介绍，具体的可以看维基百科上的定义。 朴素贝叶斯的核心是特征之间是彼此独立的，即 朴素(naive) 的含义。尽管事实上特征之间是有关联的，但朴素贝叶斯的效果通常都不错，因此它得以广泛应用。 贝叶斯定理的一般定义：假设有特征$(f_1, f_2,…,f_n)$以及类别$(C_1, C_2,…,C_m)$。要求该特征下各类别的取值概率$P(C_i|f_1,f_2,…,f_n)$。则有$$P(C_i|f_1, f_2,…,f_n)=\frac{P(C_i)\cdot P(f_1,f_2,…,f_n|C_i)}{P(f_1,f_2,…,f_n)} \tag{1}$$其中$P(C_i)$是先验概率(prior)，可以根据训练集中类目分布统计得出。$P(C_i|f_1,f_2,…,f_n)$是后验概率(posterior)$P(f_1,f_2,…,f_n|C_i)$是似然(likehood)$P(f_1,f_2,…,f_n)$是标准化常量(evidence) 公式(1)在实际应用时会有一个问题，$P(f_1,f_2,…,f_n|C_i)$这一项在计算时，当特征空间增大时，计算量会以指数级增加，造成无法计算的问题。比如每个特征有$k$个取值，则最终特征空间大小是$k^n$，这种计算复杂度是无法接受的。 朴素贝叶斯对此做了一个假设，即特征之间彼此独立。由此可得:$$P(f_1,f_2,…,f_n|C_i) \approx P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i)$$这样计算量就缩小到了$kn$量级。 朴素贝叶斯分类器根据朴素贝叶斯假设，贝叶斯定理可以简化成$$P(C_i|f_1, f_2,…,f_n)=\frac{P(C_i)P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i)}{P(f_1,f_2,…,f_n)} \tag{2}$$则要求正确的分类$$C^\ast=\underset{C_i}{Max}P(C_i|f_1,f_2,…,f_n), i=1,…,m$$对分类器来说，标准化常量项$P(f_1,f_2,…,f_n)$大家都是一样的，因此可以忽略不计，因此最终分类器为:$$C^\ast=\underset{C_i}{Max}P(C_i)P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i), i=1,…,m \tag{3}$$ 短文本分类基于朴素贝叶斯，短文本分类任务可以建模成根据词袋组合求解分类概率的问题。其中先验$P(C_i)$可以根据训练集中已有的类目分布获得。$P(f_j|C_i)$则为类目$C_i$下取得单词$f_j$的概率，可以通过计算指定类目下单词分布获得。对于NLPCC2017的新闻标题分类任务，其数据分布如下:由于只下载了train/dev数据，因此采用train部分数据来构造贝叶斯分类器，dev部分作为测试集。测试集上的部分混淆矩阵如下:可以看到，presicion跟recall分别为0.767和0.741，整体效果还是不错的。 特征工程对于文本分类任务，特征工程通常是指挑选出分类文本中权重更高的词，通过这些词来进行分类。常用的有chi方，互信息法，信息增益法，tf-idf等，这里简单介绍卡方检验和tf-idf方法。 卡方检验简介卡方检验是一种统计量在零假设成立时近似服从卡方分布的假设检验。即认为观测值跟理论值的差异是由随机误差造成的。对于理论值$E$，有一系列观测值$x_1,x_2,…,x_n$，可得归一化误差$$\sum_i{\frac{(x_i-E)^2}{E}} \tag{4}$$当误差大于一定的阈值时，认为原假设不成立。对于文本分类任务，可以假设词语$w$对分类$C$没有影响。则计算的卡方检验值越大，表明原假设越可能是错误的，即词语$w$对分类$C$有很大的影响。注：这里原假设不设置成词语$w$对分类$C$有影响的原因是，无从知晓理论值$E$，即这么假设会造成无法继续计算的问题。用一个例子说明，现有二分类任务$C={好，坏}$，样本总数为$N$，对词笨蛋计算卡方检验值。 好 坏 含“笨蛋” A B 不含“笨蛋” C D 对于上述分布，由假设，单词“笨蛋”对类别“好”没有影响。根据大数定理，理论上在总数据集中出现“笨蛋”概率为$\frac {A+B}{N}$，在类目“好”下，产生笨蛋的样本数应该是$(A+C)\frac {A+B}{N}$，即期望$E_{好,笨蛋}$。而事实上出现的数目是$A$。由此可以根据公式(4)计算得$C_{好,笨蛋}$。同理可以计算得到剩下三个值，最终的卡方检验值为:$$\chi^2(好，笨蛋)=C_{好,笨蛋}+C_{好,不含}+C_{坏,笨蛋}+C_{坏,不含}$$带入ABCD，上式可化为：$$\chi^2(word, class)=\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}$$对所有的词都进行相同操作，逆序排序取topN即前N个对该类目影响最大的词。 应用对上述NLPCC2017的短文本分类任务应用卡方检验选取特征值，对于历史类别下，特征选择结果如下：可以看到，选择结果还是很不错的。 之后对新闻标题中的词进行筛选，只保留特征选择后的结果进行分类，其中topN中的N分别取50-2000，以50为刻度。presicion变化结果如图：其中最左边是不进行特征选择，可以看到，对于短文本分类任务来说，应用卡方检验进行特征选择的效果并不好。分类器的效果随着N的增大而提高。可能原因是短文本本身信息量就很少，特征稀疏，其每个词已经是一个比较强的特征，进一步筛选后，特征变得更加稀疏，造成了效果下降。 tf-idf简介tf-idf是通过统计词在文档中的频率信息来分析词对当前文档的重要程度，和卡方检验一个很大的不同之处在于，tf-idf方法不需要事先知道当前文档的类别。 tf是指词频，即此表中每个词在当前文档中出现的频率。idf是逆向文件频率，是衡量一个词普遍重要性的指标。假设在所有文档$N$中某个词出现了$n$次，则该词的idf值为$log\frac{N}{n}$，实际计算时为了避免$n$取0，通常对其+1。 tf-idf即两者相乘。用wiki上的例子进一步说明。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。 应用还是对前面的任务，进行tf-idf特征选择，各类别top20如下：选择的效果也是非常好的。 通过tfidf作为贝叶斯分类的特征早前贝叶斯分类的权重是通过词袋计算出来的，即指定类别下各单词出现的频率。 对于训练集中的每条样本，可以计算得到该样本中个单词的tfidf值，然后累加到总的类别-tfidf矩阵中的对应位置。矩阵维度为(类目数，词表大小)。 最后对类别-tfidf矩阵进行归一化后，得到贝叶斯分类器的权重矩阵。最后的测试结果准确率为0.7533，和单纯使用bayes效果差不多。 本文涉及的所有代码保存在Github上。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
        <tag>ML</tag>
        <tag>文本分类</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[背包问题]]></title>
    <url>%2Fbag_problem%2F</url>
    <content type="text"><![CDATA[背包问题详解，如何用滚动数组优化空间复杂度。 1. 0-1背包问题问题描述： 有n个物品，它们有各自的重量和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和？ 思路: 首先定义物品的价值数组$v[n]$以及重量数组$w[n]$，以$dp[i][j]$表示背包容量为j时前i个物品的最大价值。假设已知前i-1个物品的最大价值$dp[i-1][j]$，则前i个商品最大价值取法有两种情况：1）对第i个物品，其重量$w[i]&gt;j$，则物品i无法被加到背包中，此时，$dp[i][j] = dp[i]$。2）对第i个物品，其重量$w[i]&lt;=j$，则物品i可以加到背包中，此时面临是否将其加入到背包中的决策： a.不加入，则$dp[i][j] = dp[i-1][j]$ b.加入，则$dp[i][j] = dp[i-1][j-w[i]] + v[i]$ 其中$dp[i-1][j-w[i]]$表示在背包容量为$j-w[i]$（去除商品i后还允许的重量）下，前i-1个商品的最大价值。由此: $$dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i])$$ ps. 对于上述情形2，取最大价值时会有加入/不加入两种情形的原因：直觉上，当能加入物品i时就把它加进去，价值一定会变大。但要注意的是，$dp[i][j]$定义的是背包总容量为j时前i个物品的最大价值，而不是背包剩余容量为j时。也就是说，把物品i加入背包是有代价的，代价是背包的容量减少了$w[i]$。那么之前已经在背包中的物品重量和可能会超过背包容量，因此此时要比较加入物品i能不能最大化价值。举例来说，背包中已有1个物品，价值为3，重量为3，背包总容量为4。即$dp[1][4]=3$，$dp[1][0]=0$。此时对第2a、2b个物品产生决策，若物品2a价值2，重量4，物品2b价值4，重量4。则如果把物品2a加入到背包中，需要把物品1替换，总价值反而减少成为了2。因此决策结果是不放入，而对于物品2b，决策结果是用物品2b替换物品2。2a: $dp[2a][4] = max(dp[1][4], dp[1][4-4]+v[2a]) = max(3, 2)$2b: $dp[2b][4] = max(dp[1][4], dp[1][4-4]+v[2b]) = max(3, 4)$ 12345678910111213141516171819int bag(vector&lt;int&gt; weights, vector&lt;int&gt; values, int sum) &#123; int dp[values.size()+1][sum+1]; for (int i = 0; i &lt;= sum; ++i) &#123; dp[0][i] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; dp[i][0] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; for (int j = 1; j &lt;= sum; ++j) &#123; if (j &lt; weights[i-1]) &#123; dp[i][j] = dp[i-1][j]; &#125; else &#123; dp[i][j] = max(dp[i-1][j], dp[i-1][j-weights[i-1]]+values[i-1]); &#125; &#125; &#125; return dp[values.size()][sum];&#125; 对于weights: {2,3,4,5}，values: {3,4,5,6}，容量8，有输出dp矩阵： 123456i/j 0 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 1 0 0 3 3 3 3 3 3 3 2 0 0 3 4 4 7 7 7 7 3 0 0 3 4 5 7 8 9 9 4 0 0 3 4 5 7 8 9 10 以上代码得到的是最大价值，如果要得到所有所选商品，可以从最后一个商品n开始回溯。如果$dp[i][j]=dp[i-1][j]$，表明当前第i个商品未加入背包，回到$dp[i-1][j]$，否则表明该商品在背包中，可以根据公式$dp[i][j]=dp[i-1][j-[w[i]]+v[i]$回溯到$dp[i-1][j-w[i]]$ 上述解法时间和空间复杂度都为$O(nV)$，$n$和$V$分别为物品总数和背包容量。 其中空间复杂度还可以优化到$O(V)$，利用滚动数组。状态方程为 $$dp[j]=dp[j-w[i]]+v[i]$$ 1234567891011int bag(vector&lt;int&gt; weights, vector&lt;int&gt; values, int capacity) &#123; int dp[capacity+1]; for (int i = 0; i &lt;= capacity; ++i) &#123; dp[i] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; for (int j = capacity; j &gt;= weights[i-1] ; --j) &#123; dp[j] = max(dp[j], dp[j-weights[i-1]]+values[i-1]); &#125; &#125; return dp[capacity]; 注意：遍历容量j时，要倒序遍历。原因：在更新第i个物品时，依赖$dp[i-1][j-w[i]]$这一项，因此要保证这一项只包含了前i-1个物品的情况。如果先更新了这一项，则此时背包中可能已经存在第i个物品，导致重复计算。举例来说，设第2个物品重量为2，价值为3。背包目前为空。如果顺序更新，更新$dp[2]$时有$$dp[2]=max(dp[2], dp[2-2]+3)=3$$背包中加入了物品2。更新$dp[4]$时有$$dp[4]=max(dp[4], dp[4-2]+3)=6$$物品2再次被加入到背包中，造成重复计算。 2. 完全背包问题问题描述： 完全背包问题是指，每种物品不限个数，其余条件和0-1背包问题相同。思路： 和0-1背包问题相比，区别在于将第i个物品加入到背包中后，不需要转移到$dp[i-1][j-w[i]]$的状态，而是转移到$dp[i][j-w[i]]$的状态，因为第i个物品可以反复添加。因此转移方程为: $$dp[i][j] = max(dp[i-1][j], dp[i][j-w[i]]+v[i])$$ 同样的，空间复杂度可以优化到$O(V)$，利用滚动数组得到的状态转移方程和0-1背包相同，为 $$dp[j]=dp[j-w[i]]+v[i]$$ 唯一不同之处在于，遍历容量j时要正序遍历，原因即上文所提，正序遍历时会包含重复计算已有物品，这正是完全背包问题需要的。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
        <tag>dp</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN, LSTM记录]]></title>
    <url>%2Flstm%2F</url>
    <content type="text"><![CDATA[RNN, LSTM详细介绍，以及利用tf api和手动实现两种方式复现。 RNN,LSTM介绍和实现RNNRNN有两种，一种是基于时间序列的循环神经网络Recurrent Neural Network，另一种是基于结构的递归神经网络Recursive Neural Network。我们平时讲的RNN一般情况下是指第一种。 所谓基于时间序列，是指输入的数据可以认为在时间片上是有前后关系的，即当前时间点产生的状态受到之前时间点的影响，同时会影响后续时间点的输出状态。例如文本、语音都可以看成是时间序列数据。图像在理论上不是时间序列的，但是RNN的核心是捕捉前后两个input之间的联系，而图像的像素彼此之间也是存在一定关联的，因此通过对像素构建序列，使得RNN在图像任务上也取得了一些不错的表现，比如手写识别$^{[1]}$任务中。 RNN的结构如下： 图 1 以文本任务举例，输入$(x_0,…,x_n)$就是一句话，每个$x_t$对应句子中的一个字，输入的长度就是句子的长度。如果输入的是一个batch的数据，则需要对句子的长度进行预处理，对短句子补，对长句子进行截断，确保batch中各个句子的长度是相等的。而对于每个字，可以用one-hot编码，也可以用word embedding技术进行编码。如果用word embedding，在初始化vocabulary dictionary矩阵的时候可以是使用预训练的embedding，也可以随机初始化，因为模型会在训练的时候同时把word embedding也训练出来。因此输入部分的维度就是(batch_size, seq_len, embed_dim)，对单个$x_n$来说即(batch_size, embed_dim) 再来看$A$，$A$就是一个线性变换加激活函数$$A=f(x_t, h_{t-1})=active_func(W_{hx}x_t+W_{hh}h_{t-1}+b)$$其中激活函数$active_func$可以取$tanh(\cdot)$、$relu(\cdot)$等。假设RNN中隐藏单元数为rnn_units。则$W_{hx}$的维度就是(embed_dim, rnn_units)，$W_{hh}$的维度为(rnn_units, rnn_units)，$b$的维度为(rnn_units)，计算后得到的$h_t$维度即(batch_size, rnn_units)。这个$h_t$就是RNN的输出，这个输出分两个方向，一是输出到RNN单元外，二是和下一个$x_{t+1}$一起作为下一个时间序列的输入。 NOTE: 图1中所有的$A$是同一个完整的RNN单元，里面包含了rnn_units个隐藏单元，即所有的$A$是共享参数的，因为同一个嘛。 以上就是一个完整的RNN单元，这种结构充分考虑了前后序列的信息关系，但它本质上是一种递归嵌套结构，如果不作任何处理，当时间序列长度为$n$很大时，在BPTT过程（梯度根据时间的反向传播）受到$n$次幂的影响，其值会积累到很大或衰减为0，这样就失去了之前序列的信息。因此它在处理长时间序列问题时效果不好$^{[2]}$。为了解决长依赖的问题，就有了后来的LSTM和GRU等方法。对于BPTT过程为何会产生gradient explode/gradient vanish问题，这里只提供一个直觉上的理解：设想你在读一句很长很长的话，可能长达几百上千字，当你读到最后几个字的时候，早先的记忆是不是就已经模糊了？如果想理解更多细节，具体的公式推理可以看这里$^{[3]}$。 LSTMLong-Short Term Memory简单来说，就是在之前RNN单元$A$中加了一些门控单元gate。LSTM结构如下： 经典的LSTM比起RNN，在输出隐层状态$h_t$的同时还输出了当前单元的记忆$c_t$（图中上面那个水平箭头），并且在RNN的基础上加入了三个gate，分别是： 遗忘门$f$: 控制遗忘多少前一时刻的记忆 输入门$i$: 控制当前时刻多少信息能被有效输入 输出门$o$: 控制当前时刻记忆输出多少 LSTM的核心公式为以下五个： $f_t=sigmoid(W_{fx}x_t+W_{fh}h_{t-1}+b_f)$$i_t=sigmoid(W_{ix}x_t+W_{ih}h_{t-1}+b_i)$$o_t=sigmoid(W_{ox}x_t+W_{oh}h_{t-1}+b_o)$$c_t=f_t \cdot c_{t-1}+i_t \cdot tanh(W_{cx}x_t+W_{ch}h_{t-1}+b_c)$$h_t=o_t \cdot tanh(c_t)$ 三个gate选择$sigmoid(\cdot)$的原因是gate只是负责控制信息流通率，本身是不产生额外信息的，$sigmoid(\cdot)$能很好地表现这个性质。额外信息只由自时刻$t$中的输入$(x_t,h_{t-1})$产生。LSTM更详细的工作流程可以看这篇$^{[4]}$。 同样的，为什么LSTM能够解决BPTT中的gradient exploed/gradient vanish问题，这里也只给出一个直觉上的解释：还是之前那个阅读长句子的例子，之所以读到最后记不住早前的记忆，是因为RNN尝试把所有的内容都记下来。LSTM中的gate控制着信息的流通率，可以看成只尝试去记忆之前关键的信息，这样就减少了记忆的负担，因此能很好地解决长依赖问题。如果更深入地理解BPTT过程，可以看之前提到的RNN BPTT那篇文章。 实现部分这部分用一个简单的word rebuild任务来验证LSTM结构，即训练数据的source是word，target是这个word的一个随机排序，目标是通过source预测target。模型还会用到seq2seq的相关知识，想要详细了解seq2seq的，可以看seq2seq的原始论文$^{[5]}$。实现会以Tensorflow封装好的seq2seq模块和自我实现两种方式。 数据集介绍原始数据集是小说On Sunset Highways，进行简单的预处理，去除掉连在单词后面的标点符号，提取出所有字符长度大于1的不重复单词，对各单词进行随机排序，确保source跟target单词是不同的。 利用Tensorflow中seq2seq模块实现Tensorflow为了方便用户使用，已经把基本的seq2seq组建都封装好了，使用的时候直接调用就可以，非常方便。下面简单介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import tensorflow.contrib as contribfrom tensorflow.contrib.seq2seq import *def lstm(rnn_units): return contrib.DropoutWrapper(contrib.rnn.BasicLSTMCell(rnn_units))def encoder(encoder_input, encoder_length): cell = lstm(128) _, encoder_states = tf.nn.dynamic_rnn(cell, input=encoder_input, sequence_length=encoder_length, dtype=tf.float32) return encoder_statesdef decoder(encoder_states, embedding, decoder_input, decoder_length): cell = lstm(128) output_layer = tf.layers.Dense(vocab_size) rnn_output = decoder_train(cell, decoder_input, decoder_length, encoder_states, output_layers) sample_id = decoder_infer(cell, encoder_states, output_layers, embedding) return rnn_output, sample_iddef decoder_train(cell, decoder_input, decoder_length, encoder_states, output_layer): with tf.variable_scope("decoder"): train_helper = TrainingHelper(decoder_input, decoder_length) decoder = BasicDecoder(cell, train_helper, encoder_states, output_layer) decoder_output, _, _= dynamic_decode(decoder, impute_finished=True, maximum_iterations=30) return decoder_output.rnn_outputdef decoder_infer(cell, encoder_states, output_layer, embedding): with tf.variable_scope("decoder", reuse=True): start_tokens = tf.tile(tf.constant([word2idx["&lt;SOS&gt;"]], dtype=tf.int32), [batch_size]) end_token = word2idx["&lt;EOS&gt;"] infer_helper = GreedyEmbeddingHelper(embedding, start_tokens=start_tokens, end_token=end_token) decoder = BasicDecoder(cell, infer_helper, encoder_states, output_layer) decoder_output, _, _ = dynamic_decode(decoder, impute_finished=True, maximum_iterations=30) return decoder_output.sample_id 主要分为创建lstm单元、创建encoder、创建decoder三个部分。encoder的输出encoder_states即lstm的最终输出，是最后一个时刻$t$输出的隐状态$h_t$和记忆$c_t$的组合，因为这里只用了一层LSTM，因此就是一个二元组$(c_t, h_t)$，两者的维度都是(batch_size, rnn_units)。decoder部分训练和预测的时候有些不一样，因为LSTM是根据当前状态预测下一个状态的，会有一个误差累积的过程，当序列很长时误差会累积到很大，结果是序列末的预测变得不可信。因此在训练时用了一个trick，即decoder在预测下一时刻的输出时总是用本时刻的真实输入，而不是预测产生的值，这就是Teacher Forcing方法，减轻了误差累积的影响，具体可以看这里$^{[6]}$。seq2seq模块中的TrainingHelper()已经为我们封装好了，直接用就行。预测的时候由于没有真实的target作为辅助，因此只能用生成的token作为下一时刻的输入。decoder的输出包含了rnn_output和sample_id，前者softmax后的概率分，用来计算loss，后者是预测的token，作为模型的预测输出。完整的代码可以看Github。 自主实现实现seq2seq比较简单，只要实现LSTM、全连接层output，以及encoder和decoder部分即可。先来看LSTM，根据之前的公式，只需要定义各gate的权重参数$W$和$b$，进行线性变换再激活一下就行了，这部分代码如下:12345678910# input_x: (batch_size, embed_dim)h_pre, c_pre = last_statesx = tf.concat([input_x, h_pre], axis=1) # (input_x; last_states)i, f, o, c_ = tf.split(tf.nn.xw_plus_b(x, self.W, self.b), 4, axis=1)c = tf.sigmoid(f) * c_pre + tf.sigmoid(i) * tf.tanh(c_)h = tf.sigmoid(o) * tf.tanh(c)output, states = h, (h, c)if mask is not None: output = tf.where(mask, tf.zeros_like(h), h) states = (tf.where(mask, h_pre, h), tf.where(mask, c_pre, c)) 其中mask是掩码矩阵，对于一个batch中长度较短的句子，因为之前进行了PAD处理，因此计算FP和BP时不应计算PAD部分，以免对梯度造成影响。 output部分比较简单，就不展开了。再看一下encoder和decoder部分，这里自主实现利用了Tensorflow中的TensorArray。TensorArray可以看作是装Tensor的数组，比较重要的几个方法有read，write，stack，unstack，详细用法可以看Tensorflow官方文档。实现的代码如下：1234567891011121314151617181920212223time = tf.constant(0, dtype=tf.int32)h0 = (tf.zeros([batch_size, rnn_units], dtype=tf.float32), tf.zeros([batch_size, rnn_units], dtype=tf.float32))mask = tf.zeros([batch_size], dtype=tf.bool)inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_length)inputs_ta = inputs_ta.unstack(tf.transpose(encoder_input, [1, 0, 2]))outputs_ta = tf.TensorArray(dtype=tf.float32, dynamic_size=True, size=0)def loop_func(t, x_t, s_pre, outputs_ta, mask): o_t, s_t = cell(x_t, s_pre, mask) outputs_ta = outputs_ta.write(t, o_t) mask = tf.greater_equal(t+1, encoder_input_length) x_next = tf.cond(tf.reduce_all(mask), lambda: tf.zeros([batch_size, embed_dim], dtype=tf.float32), lambda: inputs_ta.read(t+1)) return t+1, x_next, s_t, outputs_ta, mask_, _, state, output_ta, _ = tf.while_loop( cond=lambda t, _1, _2, _3, _4 : t &lt; max_length, body=loop_func, loop_vars=(time, inputs_ta.read(0), h0, outputs_ta, mask)) 实现的逻辑也很简单，对时刻$t$的输入$x_t$和前一时刻的隐层状态$h_{t-1}$执行一次LSTM计算过程，把结果写进TensorArray中，继续读入下一个时刻的输入$x_{t+1}$，判断一下是不是PAD，是的话就置0。然后重复这个过程就行了。 评测用Tensorflow提供的API训练完模型后的预测结果： 1234567source: Encinitas, predict: niscntaiE&lt;EOS&gt;source: destroying, predict: diusrntiah&lt;EOS&gt;source: tape, predict: atpe&lt;EOS&gt;source: pier, predict: rpie&lt;EOS&gt;source: unexpected, predict: teceupedxn&lt;EOS&gt;source: selecting, predict: tleinsecg&lt;EOS&gt;source: stocked, predict: cdoesctk&lt;EOS&gt; 可以看到，模型已经学到了预测规则，预测的结果基本都是输入的一个排列。 用自己实现的组建训练完模型后的预测结果：12345678Model loaded.source: Encinitas, predict: initnanh&lt;EOS&gt;source: destroying, predict: otirenidh&lt;EOS&gt;source: tape, predict: ptea&lt;EOS&gt;source: pier, predict: epir&lt;EOS&gt;source: unexpected, predict: eecdnetcm&lt;EOS&gt;source: selecting, predict: einletics&lt;EOS&gt;source: stocked, predict: eocseao&lt;EOS&gt; 可以看到，我们自己实现的组建也能够学到预测规则，但是比起Tensorflow提供的API，其预测能力要差一点，这里原因没有深入分析，如果有深入了解的，还请多多指教。 整个demo的代码以及数据我都放到了Github上，有需要的同学可以自取。 REFERENCE[1] Fast and robust training of recurrent neuralnetworks for offline handwriting recognition[2] Hochreiter (1991) [German][3] 当我们在谈论 Deep Learning：RNN 其常见架构[4] Understanding LSTM Networks[5] Sequence to Sequence Learning with Neural Networks[6] What is Teacher Forcing for Recurrent Neural Networks?]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SeqGAN]]></title>
    <url>%2FSeqGAN%2F</url>
    <content type="text"><![CDATA[SeqGAN的介绍、作用及训练过程。 GAN介绍GAN$^{[1]}$的本质是最小化生成样本分布和真实数据分布之间的差距。实现思路是交替地训练两个网络$Generator$、$Discriminator$，其中$Gen$要生成尽可能真实的样本，目标是迷惑$Dis$，使得$Dis$难以判别该样本是来自real data还是synthesized data；$Dis$的目标则显然是最大化判别样本真实与否的能力。 GAN训练过程令$x$表示real data(可以看成一张真实的图片)，$z$表示随机噪声，$D(\cdot)$表示$Discriminator$，$G(\cdot)$表示$Generator$，则$G(z)$即表示生成的假样本synthesized data(可以看成是一张生成的假图)。 GAN的目标对$D(\cdot)$而言，是最大化$D(x)$，最小化$D(G(z))$；对$G(\cdot)$而言，要最大化$D(G(z))$。以上的过程可以用一个数学公式统一表达： $$arg\underset{G}{Min}\underset{D}{Max}V(G,D)=\mathbb E_{x～P_{real}(x)} \cdot logD(x) + \mathbb E_{z～P_{z}(z)} \cdot log[1-D(G(z))] \tag{1}$$ 其中$\underset{D}{max}V(G,D)$表示$V(G,D)$最大时$D$的取值。训练时首先固定$G$，优化$D$。$V(G,D)$对$D$求导，可得最优的$D$为$$D_G^\ast=\frac{P_{real}(x)}{P_{real}(x)+P_{g}(x)}\tag{2}$$证明:对于连续空间，期望$\mathbb E$的计算方法是求积分，因此有$$\begin{eqnarray}V(G,D)&amp;=&amp;\int_x{P_{real}(x)logD(x){\rm d}x} +\int_z{P_{z}(z)log[1-D(G(z))]{\rm d}z} \tag{3}\\ &amp;=&amp;\int_x{P_{real}(x)logD(x)+P_{g}(x)log[1-D(x)]{\rm d}x} \tag{3})\end{eqnarray}$$ 将其积分符号内的部分对$D$求导，并使其为$0$。$$\frac{dV(G,D)}{dD}=\frac{P_{data}(x)}{D(x)}-\frac{P_g(x)}{1-D(x)}=0\tag{4}$$即得式$(2)$，将其带入式$(3)$得$$\begin{eqnarray}V(G,D^*)&amp;=&amp;-log(4)+KL(P_{data}||\frac{P_{data}+P_{g}}{2})+KL(P_{g}||\frac{P_{data}+P_{g}}{2}) \tag{5}\\&amp;=&amp;-log(4)+2JSD(P_{data}||P_{g}) \tag{5}\end{eqnarray}$$其中$KL(\cdot)$，$JSD(\cdot)$分别表示$KL$散度和$JS$散度，关于熵跟$KL$散度的关系，也可以点这里$^{[2]}$。 GAN的训练过程可以用下图表示$$\color{red}{\rm{IMAGE}}$$其中黑线表示真实数据分布，绿线表示生成数据分布，蓝线为$Dis$，对于蓝线而言，纵坐标可以表示$Dis$判别为真的概率。图a中是初始化的$Dis$；图b开始训练$Dis$，逐渐能开始识别出real data和synthesized data(分别给出概率为1和0)；图c开始训练$Gen$，生成的样本分布(绿线)开始向真实分布靠拢；图d是最终训练完成的理想状态，生成样本分布和真实样本分布完全重合，$Dis$的判断结果总是0.5，即无法分辨出样本的真假。 SeqGANWhy SeqGAN？为什么要引入SeqGAN呢？因为传统的GAN只能解决数据空间连续分布的问题，例如图像的生成问题。图像在计算机中是以二维矩阵的模式存储的，对一张1024*1024的黑白图片而言，就可以用1024*1024个Pixel来表示。如果图片是彩色对，只需再引入一个维度来表示RGB通道。但无论怎样，每个Pixel都是实数空间上可微的，即对某个Pixel进行一个细小的变化${\rm d}Pixel$，在原始图像上就可以表现出该像素点颜色加深/变浅之类的变化。GAN也跟传统的networks一样，使用了back propagation技术，通过对权重矩阵进行一个细微的变化来影响产生图片的质量。由于Pixel连续分布的性质，使得weight matrix的细微变化是有意义的，且能产生作用。 而对于离散分布的数据而言，以上的过程就没有意义了。如果我们假设Pixel的分布不再是连续的，而是离散的整数值。比如Pixel=0表示红色，Pixel=1表示黄色，Pixel=2表示蓝色。现在对于图片中的某个像素点而言，它可能原始值是Pixel=0，即表示一个红色的像素点。经过一轮forward propagation和back propagation后，图片的weight matrix发生了一些改变，最终结果是这个Pixel值变成了1.1。问题是，这个1.1表示什么呢？在连续空间上，它可能表示比红色深一点点，或者浅一点点，无论如何，它总是有意义的。但是在离散空间中，它就失去了意义。 再来看文本的例子，文字就是典型的离散数据，尽管有word embedding技术，给人感觉上word变成了连续的值，但事实上word在空间上的分布依然是离散的。比如“中国”的embedding是(1.1, 1.2, 1.3, …)，假设它的第一个维度变成了1.2，在word embedding空间上很可能就找不到对应的点了，即使有，它表示的含义跟“中国”也是天差地别，而非之前像素渐变的过程。 那如果硬套GAN到文本生成的任务会发生什么呢？首先来看一下整个过程： Gen输出softmax之后的结果，即对下一个词在整个词表上的概率预测，sampling一个概率最大的词，重复上述过程直到生成一个句子。Dis对该句子进行判断并优化Gen，优化的方式是通过最小化Dis的loss，然后反传回更新后的参数。问题是梯度在反传时遇到sampling这个环节就中断了（微小的变化并不能影响Sampling的结果）。那有人会说，如果不进行sampling，直接把word distribution传给$Dis$不就好了吗？这样也是不行的，这样对于$Dis$来说，他看到的real data本质上是one hot的形式，而synthesized data则是实数向量的形式，那就$Dis$很容易就判断one hot形式的是真，实数向量形式的是假，而不能真正意义上区分出real data和synthesized data。（如果这里使用了word embedding的技术，可以解决问题。）李宏毅教授在他的课程中也对这个问题作出了一些解释，他的课讲得非常好，可以看这里$^4$。 SeqGAN是怎么做的？跟普通的GAN相比，SeqGAN引入了强化学习的Policy Network。待续 REFERENCE Generative Adversarial Networks 熵和交叉熵 SeqGAN介绍 李宏毅老师的homepage]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>SeqGAN</tag>
        <tag>GAN</tag>
        <tag>RL</tag>
      </tags>
  </entry>
</search>
