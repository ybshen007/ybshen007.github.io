<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[背包问题]]></title>
    <url>%2F2018%2F09%2F24%2Fbag_problem%2F</url>
    <content type="text"><![CDATA[背包问题详解，如何用滚动数组优化空间复杂度。 1. 0-1背包问题问题描述： 有n个物品，它们有各自的重量和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和？ 思路: 首先定义物品的价值数组$v[n]$以及重量数组$w[n]$，以$dp[i][j]$表示背包容量为j时前i个物品的最大价值。假设已知前i-1个物品的最大价值$dp[i-1][j]$，则前i个商品最大价值取法有两种情况：1）对第i个物品，其重量$w[i]&gt;j$，则物品i无法被加到背包中，此时，$dp[i][j] = dp[i]$。2）对第i个物品，其重量$w[i]&lt;=j$，则物品i可以加到背包中，此时面临是否将其加入到背包中的决策： a.不加入，则$dp[i][j] = dp[i-1][j]$ b.加入，则$dp[i][j] = dp[i-1][j-w[i]] + v[i]$ 其中$dp[i-1][j-w[i]]$表示在背包容量为$j-w[i]$（去除商品i后还允许的重量）下，前i-1个商品的最大价值。由此: $$dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i])$$ ps. 对于上述情形2，取最大价值时会有加入/不加入两种情形的原因：直觉上，当能加入物品i时就把它加进去，价值一定会变大。但要注意的是，$dp[i][j]$定义的是背包总容量为j时前i个物品的最大价值，而不是背包剩余容量为j时。也就是说，把物品i加入背包是有代价的，代价是背包的容量减少了$w[i]$。那么之前已经在背包中的物品重量和可能会超过背包容量，因此此时要比较加入物品i能不能最大化价值。举例来说，背包中已有1个物品，价值为3，重量为3，背包总容量为4。即$dp[1][4]=3$，$dp[1][0]=0$。此时对第2a、2b个物品产生决策，若物品2a价值2，重量4，物品2b价值4，重量4。则如果把物品2a加入到背包中，需要把物品1替换，总价值反而减少成为了2。因此决策结果是不放入，而对于物品2b，决策结果是用物品2b替换物品2。2a: $dp[2a][4] = max(dp[1][4], dp[1][4-4]+v[2a]) = max(3, 2)$2b: $dp[2b][4] = max(dp[1][4], dp[1][4-4]+v[2b]) = max(3, 4)$ 12345678910111213141516171819int bag(vector&lt;int&gt; weights, vector&lt;int&gt; values, int sum) &#123; int dp[values.size()+1][sum+1]; for (int i = 0; i &lt;= sum; ++i) &#123; dp[0][i] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; dp[i][0] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; for (int j = 1; j &lt;= sum; ++j) &#123; if (j &lt; weights[i-1]) &#123; dp[i][j] = dp[i-1][j]; &#125; else &#123; dp[i][j] = max(dp[i-1][j], dp[i-1][j-weights[i-1]]+values[i-1]); &#125; &#125; &#125; return dp[values.size()][sum];&#125; 对于weights: {2,3,4,5}，values: {3,4,5,6}，容量8，有输出dp矩阵： 123456i/j 0 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 1 0 0 3 3 3 3 3 3 3 2 0 0 3 4 4 7 7 7 7 3 0 0 3 4 5 7 8 9 9 4 0 0 3 4 5 7 8 9 10 以上代码得到的是最大价值，如果要得到所有所选商品，可以从最后一个商品n开始回溯。如果$dp[i][j]=dp[i-1][j]$，表明当前第i个商品未加入背包，回到$dp[i-1][j]$，否则表明该商品在背包中，可以根据公式$dp[i][j]=dp[i-1][j-[w[i]]+v[i]$回溯到$dp[i-1][j-w[i]]$ 上述解法时间和空间复杂度都为$O(nV)$，$n$和$V$分别为物品总数和背包容量。 其中空间复杂度还可以优化到$O(V)$，利用滚动数组。状态方程为 $$dp[j]=dp[j-w[i]]+v[i]$$ 1234567891011int bag(vector&lt;int&gt; weights, vector&lt;int&gt; values, int capacity) &#123; int dp[capacity+1]; for (int i = 0; i &lt;= capacity; ++i) &#123; dp[i] = 0; &#125; for (int i = 1; i &lt;= values.size(); ++i) &#123; for (int j = capacity; j &gt;= weights[i-1] ; --j) &#123; dp[j] = max(dp[j], dp[j-weights[i-1]]+values[i-1]); &#125; &#125; return dp[capacity]; 注意：遍历容量j时，要倒序遍历。原因：在更新第i个物品时，依赖$dp[i-1][j-w[i]]$这一项，因此要保证这一项只包含了前i-1个物品的情况。如果先更新了这一项，则此时背包中可能已经存在第i个物品，导致重复计算。举例来说，设第2个物品重量为2，价值为3。背包目前为空。如果顺序更新，更新$dp[2]$时有$$dp[2]=max(dp[2], dp[2-2]+3)=3$$背包中加入了物品2。更新$dp[4]$时有$$dp[4]=max(dp[4], dp[4-2]+3)=6$$物品2再次被加入到背包中，造成重复计算。 2. 完全背包问题问题描述： 完全背包问题是指，每种物品不限个数，其余条件和0-1背包问题相同。思路： 和0-1背包问题相比，区别在于将第i个物品加入到背包中后，不需要转移到$dp[i-1][j-w[i]]$的状态，而是转移到$dp[i][j-w[i]]$的状态，因为第i个物品可以反复添加。因此转移方程为: $$dp[i][j] = max(dp[i-1][j], dp[i][j-w[i]]+v[i])$$ 同样的，空间复杂度可以优化到$O(V)$，利用滚动数组得到的状态转移方程和0-1背包相同，为 $$dp[j]=dp[j-w[i]]+v[i]$$ 唯一不同之处在于，遍历容量j时要正序遍历，原因即上文所提，正序遍历时会包含重复计算已有物品，这正是完全背包问题需要的。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>背包问题</tag>
        <tag>dp</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN, LSTM记录]]></title>
    <url>%2F2018%2F07%2F07%2Flstm%2F</url>
    <content type="text"><![CDATA[RNN, LSTM详细介绍，以及利用tf api和手动实现两种方式复现。 RNN,LSTM介绍和实现RNNRNN有两种，一种是基于时间序列的循环神经网络Recurrent Neural Network，另一种是基于结构的递归神经网络Recursive Neural Network。我们平时讲的RNN一般情况下是指第一种。 所谓基于时间序列，是指输入的数据可以认为在时间片上是有前后关系的，即当前时间点产生的状态受到之前时间点的影响，同时会影响后续时间点的输出状态。例如文本、语音都可以看成是时间序列数据。图像在理论上不是时间序列的，但是RNN的核心是捕捉前后两个input之间的联系，而图像的像素彼此之间也是存在一定关联的，因此通过对像素构建序列，使得RNN在图像任务上也取得了一些不错的表现，比如手写识别$^{[1]}$任务中。 RNN的结构如下： 图 1 以文本任务举例，输入$(x_0,…,x_n)$就是一句话，每个$x_t$对应句子中的一个字，输入的长度就是句子的长度。如果输入的是一个batch的数据，则需要对句子的长度进行预处理，对短句子补，对长句子进行截断，确保batch中各个句子的长度是相等的。而对于每个字，可以用one-hot编码，也可以用word embedding技术进行编码。如果用word embedding，在初始化vocabulary dictionary矩阵的时候可以是使用预训练的embedding，也可以随机初始化，因为模型会在训练的时候同时把word embedding也训练出来。因此输入部分的维度就是(batch_size, seq_len, embed_dim)，对单个$x_n$来说即(batch_size, embed_dim) 再来看$A$，$A$就是一个线性变换加激活函数$$A=f(x_t, h_{t-1})=active_func(W_{hx}x_t+W_{hh}h_{t-1}+b)$$其中激活函数$active_func$可以取$tanh(\cdot)$、$relu(\cdot)$等。假设RNN中隐藏单元数为rnn_units。则$W_{hx}$的维度就是(embed_dim, rnn_units)，$W_{hh}$的维度为(rnn_units, rnn_units)，$b$的维度为(rnn_units)，计算后得到的$h_t$维度即(batch_size, rnn_units)。这个$h_t$就是RNN的输出，这个输出分两个方向，一是输出到RNN单元外，二是和下一个$x_{t+1}$一起作为下一个时间序列的输入。 NOTE: 图1中所有的$A$是同一个完整的RNN单元，里面包含了rnn_units个隐藏单元，即所有的$A$是共享参数的，因为同一个嘛。 以上就是一个完整的RNN单元，这种结构充分考虑了前后序列的信息关系，但它本质上是一种递归嵌套结构，如果不作任何处理，当时间序列长度为$n$很大时，在BPTT过程（梯度根据时间的反向传播）受到$n$次幂的影响，其值会积累到很大或衰减为0，这样就失去了之前序列的信息。因此它在处理长时间序列问题时效果不好$^{[2]}$。为了解决长依赖的问题，就有了后来的LSTM和GRU等方法。对于BPTT过程为何会产生gradient explode/gradient vanish问题，这里只提供一个直觉上的理解：设想你在读一句很长很长的话，可能长达几百上千字，当你读到最后几个字的时候，早先的记忆是不是就已经模糊了？如果想理解更多细节，具体的公式推理可以看这里$^{[3]}$。 LSTMLong-Short Term Memory简单来说，就是在之前RNN单元$A$中加了一些门控单元gate。LSTM结构如下： 经典的LSTM比起RNN，在输出隐层状态$h_t$的同时还输出了当前单元的记忆$c_t$（图中上面那个水平箭头），并且在RNN的基础上加入了三个gate，分别是： 遗忘门$f$: 控制遗忘多少前一时刻的记忆 输入门$i$: 控制当前时刻多少信息能被有效输入 输出门$o$: 控制当前时刻记忆输出多少 LSTM的核心公式为以下五个： $f_t=sigmoid(W_{fx}x_t+W_{fh}h_{t-1}+b_f)$$i_t=sigmoid(W_{ix}x_t+W_{ih}h_{t-1}+b_i)$$o_t=sigmoid(W_{ox}x_t+W_{oh}h_{t-1}+b_o)$$c_t=o_t \cdot c_{t-1}+i_t \cdot tanh(W_{cx}x_t+W_{ch}h_{t-1}+b_c)$$h_t=o_t \cdot tanh(c_t)$ 三个gate选择$sigmoid(\cdot)$的原因是gate只是负责控制信息流通率，本身是不产生额外信息的，$sigmoid(\cdot)$能很好地表现这个性质。额外信息只由自时刻$t$中的输入$(x_t,h_{t-1})$产生。LSTM更详细的工作流程可以看这篇$^{[4]}$。 同样的，为什么LSTM能够解决BPTT中的gradient exploed/gradient vanish问题，这里也只给出一个直觉上的解释：还是之前那个阅读长句子的例子，之所以读到最后记不住早前的记忆，是因为RNN尝试把所有的内容都记下来。LSTM中的gate控制着信息的流通率，可以看成只尝试去记忆之前关键的信息，这样就减少了记忆的负担，因此能很好地解决长依赖问题。如果更深入地理解BPTT过程，可以看之前提到的RNN BPTT那篇文章。 实现部分这部分用一个简单的word rebuild任务来验证LSTM结构，即训练数据的source是word，target是这个word的一个随机排序，目标是通过source预测target。模型还会用到seq2seq的相关知识，想要详细了解seq2seq的，可以看seq2seq的原始论文$^{[5]}$。实现会以Tensorflow封装好的seq2seq模块和自我实现两种方式。 数据集介绍原始数据集是小说On Sunset Highways，进行简单的预处理，去除掉连在单词后面的标点符号，提取出所有字符长度大于1的不重复单词，对各单词进行随机排序，确保source跟target单词是不同的。 利用Tensorflow中seq2seq模块实现Tensorflow为了方便用户使用，已经把基本的seq2seq组建都封装好了，使用的时候直接调用就可以，非常方便。下面简单介绍： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import tensorflow.contrib as contribfrom tensorflow.contrib.seq2seq import *def lstm(rnn_units): return contrib.DropoutWrapper(contrib.rnn.BasicLSTMCell(rnn_units))def encoder(encoder_input, encoder_length): cell = lstm(128) _, encoder_states = tf.nn.dynamic_rnn(cell, input=encoder_input, sequence_length=encoder_length, dtype=tf.float32) return encoder_statesdef decoder(encoder_states, embedding, decoder_input, decoder_length): cell = lstm(128) output_layer = tf.layers.Dense(vocab_size) rnn_output = decoder_train(cell, decoder_input, decoder_length, encoder_states, output_layers) sample_id = decoder_infer(cell, encoder_states, output_layers, embedding) return rnn_output, sample_iddef decoder_train(cell, decoder_input, decoder_length, encoder_states, output_layer): with tf.variable_scope("decoder"): train_helper = TrainingHelper(decoder_input, decoder_length) decoder = BasicDecoder(cell, train_helper, encoder_states, output_layer) decoder_output, _, _= dynamic_decode(decoder, impute_finished=True, maximum_iterations=30) return decoder_output.rnn_outputdef decoder_infer(cell, encoder_states, output_layer, embedding): with tf.variable_scope("decoder", reuse=True): start_tokens = tf.tile(tf.constant([word2idx["&lt;SOS&gt;"]], dtype=tf.int32), [batch_size]) end_token = word2idx["&lt;EOS&gt;"] infer_helper = GreedyEmbeddingHelper(embedding, start_tokens=start_tokens, end_token=end_token) decoder = BasicDecoder(cell, infer_helper, encoder_states, output_layer) decoder_output, _, _ = dynamic_decode(decoder, impute_finished=True, maximum_iterations=30) return decoder_output.sample_id 主要分为创建lstm单元、创建encoder、创建decoder三个部分。encoder的输出encoder_states即lstm的最终输出，是最后一个时刻$t$输出的隐状态$h_t$和记忆$c_t$的组合，因为这里只用了一层LSTM，因此就是一个二元组$(c_t, h_t)$，两者的维度都是(batch_size, rnn_units)。decoder部分训练和预测的时候有些不一样，因为LSTM是根据当前状态预测下一个状态的，会有一个误差累积的过程，当序列很长时误差会累积到很大，结果是序列末的预测变得不可信。因此在训练时用了一个trick，即decoder在预测下一时刻的输出时总是用本时刻的真实输入，而不是预测产生的值，这就是Teacher Forcing方法，减轻了误差累积的影响，具体可以看这里$^{[6]}$。seq2seq模块中的TrainingHelper()已经为我们封装好了，直接用就行。预测的时候由于没有真实的target作为辅助，因此只能用生成的token作为下一时刻的输入。decoder的输出包含了rnn_output和sample_id，前者softmax后的概率分，用来计算loss，后者是预测的token，作为模型的预测输出。完整的代码可以看Github。 自主实现实现seq2seq比较简单，只要实现LSTM、全连接层output，以及encoder和decoder部分即可。先来看LSTM，根据之前的公式，只需要定义各gate的权重参数$W$和$b$，进行线性变换再激活一下就行了，这部分代码如下:12345678910# input_x: (batch_size, embed_dim)h_pre, c_pre = last_statesx = tf.concat([input_x, h_pre], axis=1) # (input_x; last_states)i, f, o, c_ = tf.split(tf.nn.xw_plus_b(x, self.W, self.b), 4, axis=1)c = tf.sigmoid(f) * c_pre + tf.sigmoid(i) * tf.tanh(c_)h = tf.sigmoid(o) * tf.tanh(c)output, states = h, (h, c)if mask is not None: output = tf.where(mask, tf.zeros_like(h), h) states = (tf.where(mask, h_pre, h), tf.where(mask, c_pre, c)) 其中mask是掩码矩阵，对于一个batch中长度较短的句子，因为之前进行了PAD处理，因此计算FP和BP时不应计算PAD部分，以免对梯度造成影响。 output部分比较简单，就不展开了。再看一下encoder和decoder部分，这里自主实现利用了Tensorflow中的TensorArray。TensorArray可以看作是装Tensor的数组，比较重要的几个方法有read，write，stack，unstack，详细用法可以看Tensorflow官方文档。实现的代码如下：1234567891011121314151617181920212223time = tf.constant(0, dtype=tf.int32)h0 = (tf.zeros([batch_size, rnn_units], dtype=tf.float32), tf.zeros([batch_size, rnn_units], dtype=tf.float32))mask = tf.zeros([batch_size], dtype=tf.bool)inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_length)inputs_ta = inputs_ta.unstack(tf.transpose(encoder_input, [1, 0, 2]))outputs_ta = tf.TensorArray(dtype=tf.float32, dynamic_size=True, size=0)def loop_func(t, x_t, s_pre, outputs_ta, mask): o_t, s_t = cell(x_t, s_pre, mask) outputs_ta = outputs_ta.write(t, o_t) mask = tf.greater_equal(t+1, encoder_input_length) x_next = tf.cond(tf.reduce_all(mask), lambda: tf.zeros([batch_size, embed_dim], dtype=tf.float32), lambda: inputs_ta.read(t+1)) return t+1, x_next, s_t, outputs_ta, mask_, _, state, output_ta, _ = tf.while_loop( cond=lambda t, _1, _2, _3, _4 : t &lt; max_length, body=loop_func, loop_vars=(time, inputs_ta.read(0), h0, outputs_ta, mask)) 实现的逻辑也很简单，对时刻$t$的输入$x_t$和前一时刻的隐层状态$h_{t-1}$执行一次LSTM计算过程，把结果写进TensorArray中，继续读入下一个时刻的输入$x_{t+1}$，判断一下是不是PAD，是的话就置0。然后重复这个过程就行了。 评测用Tensorflow提供的API训练完模型后的预测结果： 1234567source: Encinitas, predict: niscntaiE&lt;EOS&gt;source: destroying, predict: diusrntiah&lt;EOS&gt;source: tape, predict: atpe&lt;EOS&gt;source: pier, predict: rpie&lt;EOS&gt;source: unexpected, predict: teceupedxn&lt;EOS&gt;source: selecting, predict: tleinsecg&lt;EOS&gt;source: stocked, predict: cdoesctk&lt;EOS&gt; 可以看到，模型已经学到了预测规则，预测的结果基本都是输入的一个排列。 用自己实现的组建训练完模型后的预测结果：12345678Model loaded.source: Encinitas, predict: initnanh&lt;EOS&gt;source: destroying, predict: otirenidh&lt;EOS&gt;source: tape, predict: ptea&lt;EOS&gt;source: pier, predict: epir&lt;EOS&gt;source: unexpected, predict: eecdnetcm&lt;EOS&gt;source: selecting, predict: einletics&lt;EOS&gt;source: stocked, predict: eocseao&lt;EOS&gt; 可以看到，我们自己实现的组建也能够学到预测规则，但是比起Tensorflow提供的API，其预测能力要差一点，这里原因没有深入分析，如果有深入了解的，还请多多指教。 整个demo的代码以及数据我都放到了Github上，有需要的同学可以自取。 REFERENCE[1] Fast and robust training of recurrent neuralnetworks for offline handwriting recognition[2] Hochreiter (1991) [German][3] 当我们在谈论 Deep Learning：RNN 其常见架构[4] Understanding LSTM Networks[5] Sequence to Sequence Learning with Neural Networks[6] What is Teacher Forcing for Recurrent Neural Networks?]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SeqGAN]]></title>
    <url>%2F2018%2F05%2F15%2FSeqGAN%2F</url>
    <content type="text"><![CDATA[SeqGAN的介绍、作用及训练过程。 GAN介绍GAN$^{[1]}$的本质是最小化生成样本分布和真实数据分布之间的差距。实现思路是交替地训练两个网络$Generator$、$Discriminator$，其中$Gen$要生成尽可能真实的样本，目标是迷惑$Dis$，使得$Dis$难以判别该样本是来自real data还是synthesized data；$Dis$的目标则显然是最大化判别样本真实与否的能力。 GAN训练过程令$x$表示real data(可以看成一张真实的图片)，$z$表示随机噪声，$D(\cdot)$表示$Discriminator$，$G(\cdot)$表示$Generator$，则$G(z)$即表示生成的假样本synthesized data(可以看成是一张生成的假图)。 GAN的目标对$D(\cdot)$而言，是最大化$D(x)$，最小化$D(G(z))$；对$G(\cdot)$而言，要最大化$D(G(z))$。以上的过程可以用一个数学公式统一表达： $$arg\underset{G}{Min}\underset{D}{Max}V(G,D)=\mathbb E_{x～P_{real}(x)} \cdot logD(x) + \mathbb E_{z～P_{z}(z)} \cdot log[1-D(G(z))] \tag{1}$$ 其中$\underset{D}{max}V(G,D)$表示$V(G,D)$最大时$D$的取值。训练时首先固定$G$，优化$D$。$V(G,D)$对$D$求导，可得最优的$D$为$$D_G^\ast=\frac{P_{real}(x)}{P_{real}(x)+P_{g}(x)}\tag{2}$$证明:对于连续空间，期望$\mathbb E$的计算方法是求积分，因此有$$\begin{eqnarray}V(G,D)&amp;=&amp;\int_x{P_{real}(x)logD(x){\rm d}x} +\int_z{P_{z}(z)log[1-D(G(z))]{\rm d}z} \tag{3}\\ &amp;=&amp;\int_x{P_{real}(x)logD(x)+P_{g}(x)log[1-D(x)]{\rm d}x} \tag{3})\end{eqnarray}$$ 将其积分符号内的部分对$D$求导，并使其为$0$。$$\frac{dV(G,D)}{dD}=\frac{P_{data}(x)}{D(x)}-\frac{P_g(x)}{1-D(x)}=0\tag{4}$$即得式$(2)$，将其带入式$(3)$得$$\begin{eqnarray}V(G,D^*)&amp;=&amp;-log(4)+KL(P_{data}||\frac{P_{data}+P_{g}}{2})+KL(P_{g}||\frac{P_{data}+P_{g}}{2}) \tag{5}\\&amp;=&amp;-log(4)+2JSD(P_{data}||P_{g}) \tag{5}\end{eqnarray}$$其中$KL(\cdot)$，$JSD(\cdot)$分别表示$KL$散度和$JS$散度，关于熵跟$KL$散度的关系，也可以点这里$^{[2]}$。 GAN的训练过程可以用下图表示$$\color{red}{\rm{IMAGE}}$$其中黑线表示真实数据分布，绿线表示生成数据分布，蓝线为$Dis$，对于蓝线而言，纵坐标可以表示$Dis$判别为真的概率。图a中是初始化的$Dis$；图b开始训练$Dis$，逐渐能开始识别出real data和synthesized data(分别给出概率为1和0)；图c开始训练$Gen$，生成的样本分布(绿线)开始向真实分布靠拢；图d是最终训练完成的理想状态，生成样本分布和真实样本分布完全重合，$Dis$的判断结果总是0.5，即无法分辨出样本的真假。 SeqGANWhy SeqGAN？为什么要引入SeqGAN呢？因为传统的GAN只能解决数据空间连续分布的问题，例如图像的生成问题。图像在计算机中是以二维矩阵的模式存储的，对一张1024*1024的黑白图片而言，就可以用1024*1024个Pixel来表示。如果图片是彩色对，只需再引入一个维度来表示RGB通道。但无论怎样，每个Pixel都是实数空间上可微的，即对某个Pixel进行一个细小的变化${\rm d}Pixel$，在原始图像上就可以表现出该像素点颜色加深/变浅之类的变化。GAN也跟传统的networks一样，使用了back propagation技术，通过对权重矩阵进行一个细微的变化来影响产生图片的质量。由于Pixel连续分布的性质，使得weight matrix的细微变化是有意义的，且能产生作用。 而对于离散分布的数据而言，以上的过程就没有意义了。如果我们假设Pixel的分布不再是连续的，而是离散的整数值。比如Pixel=0表示红色，Pixel=1表示黄色，Pixel=2表示蓝色。现在对于图片中的某个像素点而言，它可能原始值是Pixel=0，即表示一个红色的像素点。经过一轮forward propagation和back propagation后，图片的weight matrix发生了一些改变，最终结果是这个Pixel值变成了1.1。问题是，这个1.1表示什么呢？在连续空间上，它可能表示比红色深一点点，或者浅一点点，无论如何，它总是有意义的。但是在离散空间中，它就失去了意义。 再来看文本的例子，文字就是典型的离散数据，尽管有word embedding技术，给人感觉上word变成了连续的值，但事实上word在空间上的分布依然是离散的。比如“中国”的embedding是(1.1, 1.2, 1.3, …)，假设它的第一个维度变成了1.2，在word embedding空间上很可能就找不到对应的点了，即使有，它表示的含义跟“中国”也是天差地别，而非之前像素渐变的过程。 那如果硬套GAN到文本生成的任务会发生什么呢？首先来看一下整个过程： Gen输出softmax之后的结果，即对下一个词在整个词表上的概率预测，sampling一个概率最大的词，重复上述过程直到生成一个句子。Dis对该句子进行判断并优化Gen，优化的方式是通过最小化Dis的loss，然后反传回更新后的参数。问题是梯度在反传时遇到sampling这个环节就中断了（微小的变化并不能影响Sampling的结果）。那有人会说，如果不进行sampling，直接把word distribution传给$Dis$不就好了吗？这样也是不行的，这样对于$Dis$来说，他看到的real data本质上是one hot的形式，而synthesized data则是实数向量的形式，那就$Dis$很容易就判断one hot形式的是真，实数向量形式的是假，而不能真正意义上区分出real data和synthesized data。（如果这里使用了word embedding的技术，可以解决问题。）李宏毅教授在他的课程中也对这个问题作出了一些解释，他的课讲得非常好，可以看这里$^4$。 SeqGAN是怎么做的？跟普通的GAN相比，SeqGAN引入了强化学习的Policy Network。待续 REFERENCE Generative Adversarial Networks 熵和交叉熵 SeqGAN介绍 李宏毅老师的homepage]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>SeqGAN</tag>
        <tag>GAN</tag>
        <tag>RL</tag>
      </tags>
  </entry>
</search>
