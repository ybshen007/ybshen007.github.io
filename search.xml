<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>基于朴素贝叶斯的短文本分类</title>
      <link href="/2018/10/12/bayes_tfidf/"/>
      <url>/2018/10/12/bayes_tfidf/</url>
      
        <content type="html"><![CDATA[<blockquote><p>朴素贝叶斯介绍，以及如何将它用在文本分类上。进一步地，如何引入卡方及tf-idf进行特征选择。<br><a id="more"></a></p></blockquote><h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="贝叶斯定理简介"><a href="#贝叶斯定理简介" class="headerlink" title="贝叶斯定理简介"></a>贝叶斯定理简介</h2><p>关于贝叶斯定理的文章网上已经有很多了，这里只做简单介绍，具体的可以看维基百科上的<a href="https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86" target="_blank" rel="noopener">定义</a>。</p><p>朴素贝叶斯的核心是<font color="red">特征之间是彼此独立的</font>，即 <strong>朴素(naive)</strong> 的含义。尽管事实上特征之间是有关联的，但朴素贝叶斯的效果通常都不错，因此它得以广泛应用。</p><p>贝叶斯定理的一般定义：<br>假设有特征$(f_1, f_2,…,f_n)$以及类别$(C_1, C_2,…,C_m)$。要求该特征下各类别的取值概率$P(C_i|f_1,f_2,…,f_n)$。则有<br>$$P(C_i|f_1, f_2,…,f_n)=\frac{P(C_i)\cdot P(f_1,f_2,…,f_n|C_i)}{P(f_1,f_2,…,f_n)} \tag{1}$$<br>其中<br>$P(C_i)$是先验概率(prior)，可以根据训练集中类目分布统计得出。<br>$P(C_i|f_1,f_2,…,f_n)$是后验概率(posterior)<br>$P(f_1,f_2,…,f_n|C_i)$是似然(likehood)<br>$P(f_1,f_2,…,f_n)$是标准化常量(evidence)</p><p>公式(1)在实际应用时会有一个问题，$P(f_1,f_2,…,f_n|C_i)$这一项在计算时，当特征空间增大时，计算量会以指数级增加，造成无法计算的问题。比如每个特征有$k$个取值，则最终特征空间大小是$k^n$，这种计算复杂度是无法接受的。</p><p>朴素贝叶斯对此做了一个假设，即特征之间彼此独立。由此可得:<br>$$P(f_1,f_2,…,f_n|C_i) \approx P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i)$$<br>这样计算量就缩小到了$kn$量级。</p><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>根据朴素贝叶斯假设，贝叶斯定理可以简化成<br>$$P(C_i|f_1, f_2,…,f_n)=\frac{P(C_i)P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i)}{P(f_1,f_2,…,f_n)} \tag{2}$$<br>则要求正确的分类<br>$$C^\ast=\underset{C_i}{Max}P(C_i|f_1,f_2,…,f_n), i=1,…,m$$<br>对分类器来说，标准化常量项$P(f_1,f_2,…,f_n)$大家都是一样的，因此可以忽略不计，因此最终分类器为:<br>$$C^\ast=\underset{C_i}{Max}P(C_i)P(f_1|C_i)P(f_2|C_i)…P(f_n|C_i), i=1,…,m \tag{3}$$</p><h1 id="短文本分类"><a href="#短文本分类" class="headerlink" title="短文本分类"></a>短文本分类</h1><p>基于朴素贝叶斯，短文本分类任务可以建模成根据词袋组合求解分类概率的问题。其中先验$P(C_i)$可以根据训练集中已有的类目分布获得。$P(f_j|C_i)$则为类目$C_i$下取得单词$f_j$的概率，可以通过计算指定类目下单词分布获得。<br>对于NLPCC2017的新闻标题分类<a href="http://tcci.ccf.org.cn/conference/2017/dldoc/taskgline02.pdf" target="_blank" rel="noopener">任务</a>，其数据分布如下:<br><img src="/images/data.jpg" alt="数据"><br>由于只下载了train/dev数据，因此采用train部分数据来构造贝叶斯分类器，dev部分作为测试集。测试集上的部分混淆矩阵如下:<br><img src="/images/matrix.jpg" alt="matrix"><br>可以看到，presicion跟recall分别为0.767和0.741，整体效果还是不错的。</p><h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>对于文本分类任务，特征工程通常是指挑选出分类文本中权重更高的词，通过这些词来进行分类。常用的有chi方，互信息法，信息增益法，tf-idf等，这里简单介绍卡方检验和tf-idf方法。</p><h2 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>卡方检验是一种统计量在零假设成立时近似服从卡方分布的假设检验。即认为观测值跟理论值的差异是由随机误差造成的。<br>对于理论值$E$，有一系列观测值$x_1,x_2,…,x_n$，可得归一化误差<br>$$\sum_i{\frac{(x_i-E)^2}{E}} \tag{4}$$<br>当误差大于一定的阈值时，认为原假设不成立。<br>对于文本分类任务，可以假设<font color="red">词语$w$对分类$C$没有影响</font>。则计算的卡方检验值越大，表明原假设越可能是错误的，即词语$w$对分类$C$有很大的影响。<br><strong>注：这里原假设不设置成词语$w$对分类$C$有影响的原因是，无从知晓理论值$E$，即这么假设会造成无法继续计算的问题。</strong><br>用一个例子说明，现有二分类任务$C={好，坏}$，样本总数为$N$，对词<em>笨蛋</em>计算卡方检验值。</p><table><thead><tr><th></th><th style="text-align:center">好</th><th style="text-align:center">坏</th></tr></thead><tbody><tr><td><strong>含“笨蛋”</strong></td><td style="text-align:center">A</td><td style="text-align:center">B</td></tr><tr><td><strong>不含“笨蛋”</strong></td><td style="text-align:center">C</td><td style="text-align:center">D</td></tr></tbody></table><p>对于上述分布，由假设，单词“笨蛋”对类别“好”没有影响。根据大数定理，理论上在总数据集中出现“笨蛋”概率为$\frac {A+B}{N}$，在类目“好”下，产生笨蛋的样本数应该是$(A+C)\frac {A+B}{N}$，即期望$E_{好,笨蛋}$。而事实上出现的数目是$A$。由此可以根据公式(4)计算得$C_{好,笨蛋}$。同理可以计算得到剩下三个值，最终的卡方检验值为:<br>$$\chi^2(好，笨蛋)=C_{好,笨蛋}+C_{好,不含}+C_{坏,笨蛋}+C_{坏,不含}$$<br>带入ABCD，上式可化为：<br>$$\chi^2(word, class)=\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}$$<br>对所有的词都进行相同操作，逆序排序取topN即前N个对该类目影响最大的词。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>对上述NLPCC2017的短文本分类任务应用卡方检验选取特征值，对于历史类别下，特征选择结果如下：<br><img src="/images/chi.jpg" alt="chi"><br>可以看到，选择结果还是很不错的。</p><p>之后对新闻标题中的词进行筛选，只保留特征选择后的结果进行分类，其中topN中的N分别取50-2000，以50为刻度。presicion变化结果如图：<br><img src="/images/presicion.jpg" alt="presicion"><br>其中最左边是不进行特征选择，可以看到，对于短文本分类任务来说，应用卡方检验进行特征选择的效果并不好。分类器的效果随着N的增大而提高。可能原因是短文本本身信息量就很少，特征稀疏，其每个词已经是一个比较强的特征，进一步筛选后，特征变得更加稀疏，造成了效果下降。</p><h2 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>tf-idf是通过统计词在文档中的频率信息来分析词对当前文档的重要程度，和卡方检验一个很大的不同之处在于，tf-idf方法不需要事先知道当前文档的类别。</p><p>tf是指词频，即此表中每个词在当前文档中出现的频率。<br>idf是逆向文件频率，是衡量一个词普遍重要性的指标。假设在所有文档$N$中某个词出现了$n$次，则该词的idf值为$log\frac{N}{n}$，实际计算时为了避免$n$取0，通常对其+1。</p><p>tf-idf即两者相乘。用<a href="https://zh.wikipedia.org/wiki/Tf-idf" target="_blank" rel="noopener">wiki</a>上的例子进一步说明。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。</p><h3 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h3><p>还是对前面的任务，进行tf-idf特征选择，各类别top20如下：<br><img src="/images/tfidf.jpg" alt="tfidf"><br>选择的效果也是非常好的。</p><h3 id="通过tfidf作为贝叶斯分类的特征"><a href="#通过tfidf作为贝叶斯分类的特征" class="headerlink" title="通过tfidf作为贝叶斯分类的特征"></a>通过tfidf作为贝叶斯分类的特征</h3><p>早前贝叶斯分类的权重是通过词袋计算出来的，即指定类别下各单词出现的频率。</p><p>对于训练集中的每条样本，可以计算得到该样本中个单词的tfidf值，然后累加到总的类别-tfidf矩阵中的对应位置。矩阵维度为(类目数，词表大小)。</p><p>最后对类别-tfidf矩阵进行归一化后，得到贝叶斯分类器的权重矩阵。最后的测试结果准确率为0.7533，和单纯使用bayes效果差不多。</p><p>本文涉及的所有代码保存在<a href="https://github.com/ybshen007/classification" target="_blank" rel="noopener">Github</a>上。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朴素贝叶斯 </tag>
            
            <tag> ML </tag>
            
            <tag> 文本分类 </tag>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>背包问题</title>
      <link href="/2018/09/24/bag_problem/"/>
      <url>/2018/09/24/bag_problem/</url>
      
        <content type="html"><![CDATA[<blockquote><p>背包问题详解，如何用滚动数组优化空间复杂度。<br><a id="more"></a></p></blockquote><h2 id="1-0-1背包问题"><a href="#1-0-1背包问题" class="headerlink" title="1. 0-1背包问题"></a>1. 0-1背包问题</h2><p><strong>问题描述：</strong> 有n个物品，它们有各自的重量和价值，现有给定容量的背包，如何让背包里装入的物品具有最大的价值总和？  </p><p><strong>思路:</strong> 首先定义物品的价值数组$v[n]$以及重量数组$w[n]$，以$dp[i][j]$表示<strong>背包容量为j</strong>时<strong>前i个</strong>物品的最大价值。假设已知前i-1个物品的最大价值$dp[i-1][j]$，则前i个商品最大价值取法有两种情况：<br>1）对第i个物品，其重量$w[i]&gt;j$，则物品i无法被加到背包中，此时，$dp[i][j] = dp[i]$。<br>2）对第i个物品，其重量$w[i]&lt;=j$，则物品i可以加到背包中，此时面临是否将其加入到背包中的决策：<br>　　a.不加入，则$dp[i][j] = dp[i-1][j]$<br>　　b.加入，则$dp[i][j] = dp[i-1][j-w[i]] + v[i]$  </p><p>其中$dp[i-1][j-w[i]]$表示在背包容量为$j-w[i]$（去除商品i后还允许的重量）下，前i-1个商品的最大价值。由此:  </p><p>$$dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+v[i])$$  </p><p>ps. 对于上述情形2，取最大价值时会有加入/不加入两种情形的原因：<br>直觉上，当能加入物品i时就把它加进去，价值一定会变大。但要注意的是，$dp[i][j]$定义的是背包<strong>总容量</strong>为j时前i个物品的最大价值，而不是背包<strong>剩余容量</strong>为j时。也就是说，把物品i加入背包是有代价的，代价是背包的容量减少了$w[i]$。那么之前已经在背包中的物品重量和可能会超过背包容量，因此此时要比较加入物品i能不能最大化价值。举例来说，背包中已有1个物品，价值为3，重量为3，背包总容量为4。即$dp[1][4]=3$，$dp[1][0]=0$。此时对第2a、2b个物品产生决策，若物品2a价值2，重量4，物品2b价值4，重量4。则如果把物品2a加入到背包中，需要把物品1替换，总价值反而减少成为了2。因此决策结果是不放入，而对于物品2b，决策结果是用物品2b替换物品2。<br>2a: $dp[2a][4] = max(dp[1][4], dp[1][4-4]+v[2a]) = max(3, 2)$<br>2b: $dp[2b][4] = max(dp[1][4], dp[1][4-4]+v[2b]) = max(3, 4)$  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bag</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[values.size()+<span class="number">1</span>][sum+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= sum; ++i) &#123;</span><br><span class="line">        dp[<span class="number">0</span>][i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= values.size(); ++i) &#123;</span><br><span class="line">        dp[i][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= values.size(); ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= sum; ++j) &#123;</span><br><span class="line">            <span class="keyword">if</span> (j &lt; weights[i<span class="number">-1</span>]) &#123;</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j];</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i<span class="number">-1</span>][j-weights[i<span class="number">-1</span>]]+values[i<span class="number">-1</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[values.size()][sum];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于weights: {2,3,4,5}，values: {3,4,5,6}，容量8，有输出dp矩阵：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">i/j 0 1 2 3 4 5 6 7 8</span><br><span class="line"> 0  0 0 0 0 0 0 0 0 0</span><br><span class="line"> 1  0 0 3 3 3 3 3 3 3</span><br><span class="line"> 2  0 0 3 4 4 7 7 7 7</span><br><span class="line"> 3  0 0 3 4 5 7 8 9 9</span><br><span class="line"> 4  0 0 3 4 5 7 8 9 10</span><br></pre></td></tr></table></figure><p>以上代码得到的是最大价值，如果要得到所有所选商品，可以从最后一个商品n开始回溯。如果$dp[i][j]=dp[i-1][j]$，表明当前第i个商品未加入背包，回到$dp[i-1][j]$，否则表明该商品在背包中，可以根据公式$dp[i][j]=dp[i-1][j-[w[i]]+v[i]$回溯到$dp[i-1][j-w[i]]$  </p><p>上述解法时间和空间复杂度都为$O(nV)$，$n$和$V$分别为物品总数和背包容量。 其中空间复杂度还可以优化到$O(V)$，利用滚动数组。状态方程为  </p><p>$$dp[j]=dp[j-w[i]]+v[i]$$  </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bag</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; weights, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values, <span class="keyword">int</span> capacity)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[capacity+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= capacity; ++i) &#123;</span><br><span class="line">        dp[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= values.size(); ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = capacity; j &gt;= weights[i<span class="number">-1</span>] ; --j) &#123;</span><br><span class="line">            dp[j] = max(dp[j], dp[j-weights[i<span class="number">-1</span>]]+values[i<span class="number">-1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[capacity];</span><br></pre></td></tr></table></figure><p><strong>注意：遍历容量j时，要倒序遍历。</strong><br>原因：在更新第i个物品时，依赖$dp[i-1][j-w[i]]$这一项，因此要保证这一项只包含了前i-1个物品的情况。如果先更新了这一项，则此时背包中可能已经存在第i个物品，导致重复计算。<br>举例来说，设第2个物品重量为2，价值为3。背包目前为空。如果顺序更新，更新$dp[2]$时有<br>$$dp[2]=max(dp[2], dp[2-2]+3)=3$$<br>背包中加入了物品2。更新$dp[4]$时有<br>$$dp[4]=max(dp[4], dp[4-2]+3)=6$$<br>物品2再次被加入到背包中，造成重复计算。  </p><h2 id="2-完全背包问题"><a href="#2-完全背包问题" class="headerlink" title="2. 完全背包问题"></a>2. 完全背包问题</h2><p><strong>问题描述：</strong> 完全背包问题是指，每种物品不限个数，其余条件和0-1背包问题相同。<br><strong>思路：</strong> 和0-1背包问题相比，区别在于将第i个物品加入到背包中后，不需要转移到$dp[i-1][j-w[i]]$的状态，而是转移到$dp[i][j-w[i]]$的状态，因为第i个物品可以反复添加。因此转移方程为:  </p><p>$$dp[i][j] = max(dp[i-1][j], dp[i][j-w[i]]+v[i])$$</p><p>同样的，空间复杂度可以优化到$O(V)$，利用滚动数组得到的状态转移方程和0-1背包相同，为  </p><p>$$dp[j]=dp[j-w[i]]+v[i]$$</p><p>唯一不同之处在于，遍历容量j时要<strong>正序</strong>遍历，原因即上文所提，正序遍历时会包含重复计算已有物品，这正是完全背包问题需要的。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 背包问题 </tag>
            
            <tag> dp </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RNN, LSTM记录</title>
      <link href="/2018/07/07/lstm/"/>
      <url>/2018/07/07/lstm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>RNN, LSTM详细介绍，以及利用tf api和手动实现两种方式复现。<br><a id="more"></a></p></blockquote><h1 id="RNN-LSTM介绍和实现"><a href="#RNN-LSTM介绍和实现" class="headerlink" title="RNN,LSTM介绍和实现"></a>RNN,LSTM介绍和实现</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>RNN有两种，一种是基于时间序列的循环神经网络Recurrent Neural Network，另一种是基于结构的递归神经网络Recursive Neural Network。我们平时讲的RNN一般情况下是指第一种。  </p><p>所谓基于时间序列，是指输入的数据可以认为在时间片上是有前后关系的，即当前时间点产生的状态受到之前时间点的影响，同时会影响后续时间点的输出状态。例如文本、语音都可以看成是时间序列数据。图像在理论上不是时间序列的，但是RNN的核心是捕捉前后两个input之间的联系，而图像的像素彼此之间也是存在一定关联的，因此通过对像素构建序列，使得RNN在图像任务上也取得了一些不错的表现，比如手写识别$^{[1]}$任务中。  </p><p>RNN的结构如下： </p><!-- <div align=center> --><p><img src="https://i.loli.net/2018/08/04/5b654b0b9d8ba.png" alt="unrolled_rnn"> <!--/div --> </p><center>图 1</center><p>以文本任务举例，输入$(x_0,…,x_n)$就是一句话，每个$x_t$对应句子中的一个字，输入的长度就是句子的长度。如果输入的是一个batch的数据，则需要对句子的长度进行预处理，对短句子补<pad>，对长句子进行截断，确保batch中各个句子的长度是相等的。而对于每个字，可以用one-hot编码，也可以用word embedding技术进行编码。如果用word embedding，在初始化vocabulary dictionary矩阵的时候可以是使用预训练的embedding，也可以随机初始化，因为模型会在训练的时候同时把word embedding也训练出来。因此输入部分的维度就是<code>(batch_size, seq_len, embed_dim)</code>，对单个$x_n$来说即<code>(batch_size, embed_dim)</code>  </pad></p><p>再来看$A$，$A$就是一个线性变换加激活函数<br>$$A=f(x_t, h_{t-1})=active_func(W_{hx}x_t+W_{hh}h_{t-1}+b)$$<br>其中激活函数$active_func$可以取$tanh(\cdot)$、$relu(\cdot)$等。假设RNN中隐藏单元数为<code>rnn_units</code>。则$W_{hx}$的维度就是<code>(embed_dim, rnn_units)</code>，$W_{hh}$的维度为<code>(rnn_units, rnn_units)</code>，$b$的维度为<code>(rnn_units)</code>，计算后得到的$h_t$维度即<code>(batch_size, rnn_units)</code>。这个$h_t$就是RNN的输出，这个输出分两个方向，一是输出到RNN单元外，二是和下一个$x_{t+1}$一起作为下一个时间序列的输入。  </p><p><strong>NOTE:</strong> 图1中<strong>所有</strong>的$A$是<strong>同一个</strong>完整的RNN单元，里面包含了<code>rnn_units</code>个隐藏单元，即所有的$A$是共享参数的，因为同一个嘛。  </p><p>以上就是一个完整的RNN单元，这种结构充分考虑了前后序列的信息关系，但它本质上是一种递归嵌套结构，如果不作任何处理，当时间序列长度为$n$很大时，在BPTT过程（梯度根据时间的反向传播）受到$n$次幂的影响，其值会积累到很大或衰减为0，这样就失去了之前序列的信息。因此它在处理长时间序列问题时效果不好$^{[2]}$。为了解决长依赖的问题，就有了后来的LSTM和GRU等方法。对于BPTT过程为何会产生gradient explode/gradient vanish问题，这里只提供一个直觉上的理解：设想你在读一句很长很长的话，可能长达几百上千字，当你读到最后几个字的时候，早先的记忆是不是就已经模糊了？如果想理解更多细节，具体的公式推理可以看<a href="https://zhuanlan.zhihu.com/p/27485750" target="_blank" rel="noopener">这里$^{[3]}$</a>。</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long-Short Term Memory简单来说，就是在之前RNN单元$A$中加了一些门控单元gate。LSTM结构如下：<br><!-- <div align=center> --></p><p><img src="https://i.loli.net/2018/08/04/5b655b8e0a24e.png" alt="lstm_cell"><br><!-- </div> --></p><p>经典的LSTM比起RNN，在输出隐层状态$h_t$的同时还输出了当前单元的记忆$c_t$（图中上面那个水平箭头），并且在RNN的基础上加入了三个gate，分别是：  </p><ul><li>遗忘门$f$: 控制遗忘多少前一时刻的记忆</li><li>输入门$i$: 控制当前时刻多少信息能被有效输入</li><li>输出门$o$: 控制当前时刻记忆输出多少  </li></ul><p>LSTM的核心公式为以下五个：</p><center><br>$f_t=sigmoid(W_{fx}x_t+W_{fh}h_{t-1}+b_f)$<br>$i_t=sigmoid(W_{ix}x_t+W_{ih}h_{t-1}+b_i)$<br>$o_t=sigmoid(W_{ox}x_t+W_{oh}h_{t-1}+b_o)$<br>$c_t=o_t \cdot c_{t-1}+i_t \cdot tanh(W_{cx}x_t+W_{ch}h_{t-1}+b_c)$<br>$h_t=o_t \cdot tanh(c_t)$<br></center><p>三个gate选择$sigmoid(\cdot)$的原因是gate只是负责控制信息<strong>流通率</strong>，本身是不产生额外信息的，$sigmoid(\cdot)$能很好地表现这个性质。额外信息只由自时刻$t$中的输入$(x_t,h_{t-1})$产生。LSTM更详细的工作流程可以看<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">这篇$^{[4]}$</a>。  </p><p>同样的，为什么LSTM能够解决BPTT中的gradient exploed/gradient vanish问题，这里也只给出一个直觉上的解释：还是之前那个阅读长句子的例子，之所以读到最后记不住早前的记忆，是因为RNN尝试把所有的内容都记下来。LSTM中的gate控制着信息的流通率，可以看成只尝试去记忆之前关键的信息，这样就减少了记忆的负担，因此能很好地解决长依赖问题。如果更深入地理解BPTT过程，可以看之前提到的RNN BPTT那篇文章。  </p><h2 id="实现部分"><a href="#实现部分" class="headerlink" title="实现部分"></a>实现部分</h2><p>这部分用一个简单的word rebuild任务来验证LSTM结构，即训练数据的source是word，target是这个word的一个随机排序，目标是通过source预测target。模型还会用到seq2seq的相关知识，想要详细了解seq2seq的，可以看seq2seq的<a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">原始论文$^{[5]}$</a>。实现会以Tensorflow封装好的seq2seq模块和自我实现两种方式。</p><h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p>原始数据集是小说<em>On Sunset Highways</em>，进行简单的预处理，去除掉连在单词后面的标点符号，提取出所有字符长度大于1的不重复单词，对各单词进行随机排序，确保source跟target单词是不同的。  </p><h3 id="利用Tensorflow中seq2seq模块实现"><a href="#利用Tensorflow中seq2seq模块实现" class="headerlink" title="利用Tensorflow中seq2seq模块实现"></a>利用Tensorflow中seq2seq模块实现</h3><p>Tensorflow为了方便用户使用，已经把基本的seq2seq组建都封装好了，使用的时候直接调用就可以，非常方便。下面简单介绍：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib <span class="keyword">as</span> contrib</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.seq2seq <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span><span class="params">(rnn_units)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> contrib.DropoutWrapper(contrib.rnn.BasicLSTMCell(rnn_units))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encoder</span><span class="params">(encoder_input, encoder_length)</span>:</span></span><br><span class="line">    cell = lstm(<span class="number">128</span>)</span><br><span class="line">    _, encoder_states = tf.nn.dynamic_rnn(cell,</span><br><span class="line">                                          input=encoder_input,</span><br><span class="line">                                          sequence_length=encoder_length,</span><br><span class="line">                                          dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> encoder_states</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder</span><span class="params">(encoder_states, embedding, decoder_input, decoder_length)</span>:</span></span><br><span class="line">    cell = lstm(<span class="number">128</span>)</span><br><span class="line">    output_layer = tf.layers.Dense(vocab_size)</span><br><span class="line">    rnn_output = decoder_train(cell, decoder_input, decoder_length, encoder_states, output_layers)</span><br><span class="line">    sample_id = decoder_infer(cell, encoder_states, output_layers, embedding)</span><br><span class="line">    <span class="keyword">return</span> rnn_output, sample_id</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_train</span><span class="params">(cell, decoder_input, decoder_length, encoder_states, output_layer)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>):</span><br><span class="line">        train_helper = TrainingHelper(decoder_input, decoder_length)</span><br><span class="line">        decoder = BasicDecoder(cell, </span><br><span class="line">                               train_helper, </span><br><span class="line">                               encoder_states, </span><br><span class="line">                               output_layer)</span><br><span class="line">        decoder_output, _, _= dynamic_decode(decoder,</span><br><span class="line">                                             impute_finished=<span class="keyword">True</span>, </span><br><span class="line">                                             maximum_iterations=<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">return</span> decoder_output.rnn_output</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decoder_infer</span><span class="params">(cell, encoder_states, output_layer, embedding)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">        start_tokens = tf.tile(tf.constant([word2idx[<span class="string">"&lt;SOS&gt;"</span>]], dtype=tf.int32), [batch_size])</span><br><span class="line">        end_token = word2idx[<span class="string">"&lt;EOS&gt;"</span>]</span><br><span class="line">        infer_helper = GreedyEmbeddingHelper(embedding,</span><br><span class="line">                                             start_tokens=start_tokens,</span><br><span class="line">                                             end_token=end_token)</span><br><span class="line">        decoder = BasicDecoder(cell, infer_helper, encoder_states, output_layer)</span><br><span class="line">        decoder_output, _, _ = dynamic_decode(decoder,</span><br><span class="line">                                              impute_finished=<span class="keyword">True</span>, </span><br><span class="line">                                              maximum_iterations=<span class="number">30</span>)</span><br><span class="line">    <span class="keyword">return</span> decoder_output.sample_id</span><br></pre></td></tr></table></figure><p>主要分为创建lstm单元、创建encoder、创建decoder三个部分。encoder的输出encoder_states即lstm的最终输出，是最后一个时刻$t$输出的隐状态$h_t$和记忆$c_t$的组合，因为这里只用了一层LSTM，因此就是一个二元组$(c_t, h_t)$，两者的维度都是<code>(batch_size, rnn_units)</code>。<br>decoder部分训练和预测的时候有些不一样，因为LSTM是根据当前状态预测下一个状态的，会有一个误差累积的过程，当序列很长时误差会累积到很大，结果是序列末的预测变得不可信。因此在训练时用了一个trick，即decoder在预测下一时刻的输出时总是用本时刻的真实输入，而不是预测产生的值，这就是Teacher Forcing方法，减轻了误差累积的影响，具体可以看<a href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" target="_blank" rel="noopener">这里$^{[6]}$</a>。seq2seq模块中的TrainingHelper()已经为我们封装好了，直接用就行。预测的时候由于没有真实的target作为辅助，因此只能用生成的token作为下一时刻的输入。decoder的输出包含了rnn_output和sample_id，前者softmax后的概率分，用来计算loss，后者是预测的token，作为模型的预测输出。完整的代码可以看<a href="">Github</a>。  </p><h3 id="自主实现"><a href="#自主实现" class="headerlink" title="自主实现"></a>自主实现</h3><p>实现seq2seq比较简单，只要实现LSTM、全连接层output，以及encoder和decoder部分即可。先来看LSTM，根据之前的公式，只需要定义各gate的权重参数$W$和$b$，进行线性变换再激活一下就行了，这部分代码如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># input_x: (batch_size, embed_dim)</span></span><br><span class="line">h_pre, c_pre = last_states</span><br><span class="line">x = tf.concat([input_x, h_pre], axis=<span class="number">1</span>)   <span class="comment"># (input_x; last_states)</span></span><br><span class="line">i, f, o, c_ = tf.split(tf.nn.xw_plus_b(x, self.W, self.b), <span class="number">4</span>, axis=<span class="number">1</span>)</span><br><span class="line">c = tf.sigmoid(f) * c_pre + tf.sigmoid(i) * tf.tanh(c_)</span><br><span class="line">h = tf.sigmoid(o) * tf.tanh(c)</span><br><span class="line">output, states = h, (h, c)</span><br><span class="line"><span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    output = tf.where(mask, tf.zeros_like(h), h)</span><br><span class="line">    states = (tf.where(mask, h_pre, h), tf.where(mask, c_pre, c))</span><br></pre></td></tr></table></figure></p><p>其中<code>mask</code>是掩码矩阵，对于一个batch中长度较短的句子，因为之前进行了PAD处理，因此计算FP和BP时不应计算PAD部分，以免对梯度造成影响。  </p><p>output部分比较简单，就不展开了。再看一下encoder和decoder部分，这里自主实现利用了Tensorflow中的TensorArray。TensorArray可以看作是装Tensor的数组，比较重要的几个方法有read，write，stack，unstack，详细用法可以看Tensorflow<a href="https://www.tensorflow.org/api_docs/python/tf/TensorArray" target="_blank" rel="noopener">官方文档</a>。实现的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">time = tf.constant(<span class="number">0</span>, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">h0 = (tf.zeros([batch_size, rnn_units], dtype=tf.float32),</span><br><span class="line">        tf.zeros([batch_size, rnn_units], dtype=tf.float32))</span><br><span class="line">mask = tf.zeros([batch_size], dtype=tf.bool)</span><br><span class="line">inputs_ta = tf.TensorArray(dtype=tf.float32, size=max_length)</span><br><span class="line">inputs_ta = inputs_ta.unstack(tf.transpose(encoder_input, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line">outputs_ta = tf.TensorArray(dtype=tf.float32, dynamic_size=<span class="keyword">True</span>, size=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop_func</span><span class="params">(t, x_t, s_pre, outputs_ta, mask)</span>:</span></span><br><span class="line">    o_t, s_t = cell(x_t, s_pre, mask)</span><br><span class="line">    outputs_ta = outputs_ta.write(t, o_t)</span><br><span class="line">    mask = tf.greater_equal(t+<span class="number">1</span>, encoder_input_length)</span><br><span class="line">    x_next = tf.cond(tf.reduce_all(mask),</span><br><span class="line">                        <span class="keyword">lambda</span>: tf.zeros([batch_size, embed_dim], dtype=tf.float32),</span><br><span class="line">                        <span class="keyword">lambda</span>: inputs_ta.read(t+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> t+<span class="number">1</span>, x_next, s_t, outputs_ta, mask</span><br><span class="line"></span><br><span class="line">_, _, state, output_ta, _ = tf.while_loop(</span><br><span class="line">    cond=<span class="keyword">lambda</span> t, _1, _2, _3, _4 : t &lt; max_length,</span><br><span class="line">    body=loop_func,</span><br><span class="line">    loop_vars=(time, inputs_ta.read(<span class="number">0</span>), h0, outputs_ta, mask)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>实现的逻辑也很简单，对时刻$t$的输入$x_t$和前一时刻的隐层状态$h_{t-1}$执行一次LSTM计算过程，把结果写进TensorArray中，继续读入下一个时刻的输入$x_{t+1}$，判断一下是不是PAD，是的话就置0。然后重复这个过程就行了。  </p><h3 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h3><p>用Tensorflow提供的API训练完模型后的预测结果： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source: Encinitas, predict: niscntaiE&lt;EOS&gt;</span><br><span class="line">source: destroying, predict: diusrntiah&lt;EOS&gt;</span><br><span class="line">source: tape, predict: atpe&lt;EOS&gt;</span><br><span class="line">source: pier, predict: rpie&lt;EOS&gt;</span><br><span class="line">source: unexpected, predict: teceupedxn&lt;EOS&gt;</span><br><span class="line">source: selecting, predict: tleinsecg&lt;EOS&gt;</span><br><span class="line">source: stocked, predict: cdoesctk&lt;EOS&gt;</span><br></pre></td></tr></table></figure><p>可以看到，模型已经学到了预测规则，预测的结果基本都是输入的一个排列。  </p><p>用自己实现的组建训练完模型后的预测结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Model loaded.</span><br><span class="line">source: Encinitas, predict: initnanh&lt;EOS&gt;</span><br><span class="line">source: destroying, predict: otirenidh&lt;EOS&gt;</span><br><span class="line">source: tape, predict: ptea&lt;EOS&gt;</span><br><span class="line">source: pier, predict: epir&lt;EOS&gt;</span><br><span class="line">source: unexpected, predict: eecdnetcm&lt;EOS&gt;</span><br><span class="line">source: selecting, predict: einletics&lt;EOS&gt;</span><br><span class="line">source: stocked, predict: eocseao&lt;EOS&gt;</span><br></pre></td></tr></table></figure></p><p>可以看到，我们自己实现的组建也能够学到预测规则，但是比起Tensorflow提供的API，其预测能力要差一点，这里原因没有深入分析，如果有深入了解的，还请多多指教。  </p><p>整个demo的代码以及数据我都放到了<a href="https://github.com/ybshen007/tensorflow-practice/tree/master/rnn" target="_blank" rel="noopener">Github</a>上，有需要的同学可以自取。</p><h2 id="REFERENCE"><a href="#REFERENCE" class="headerlink" title="REFERENCE"></a>REFERENCE</h2><p><a href="http://www.icfhr2014.org/wp-content/uploads/2015/02/ICFHR2014-Doetsch.pdf" target="_blank" rel="noopener">[1] Fast and robust training of recurrent neuralnetworks for offline handwriting recognition</a><br><a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">[2] Hochreiter (1991) [German]</a><br><a href="https://zhuanlan.zhihu.com/p/27485750" target="_blank" rel="noopener">[3] 当我们在谈论 Deep Learning：RNN 其常见架构</a><br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">[4] Understanding LSTM Networks</a><br><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">[5] Sequence to Sequence Learning with Neural Networks</a><br><a href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" target="_blank" rel="noopener">[6] What is Teacher Forcing for Recurrent Neural Networks?</a></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SeqGAN</title>
      <link href="/2018/05/15/SeqGAN/"/>
      <url>/2018/05/15/SeqGAN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>SeqGAN的介绍、作用及训练过程。<br><a id="more"></a></p></blockquote><h2 id="GAN介绍"><a href="#GAN介绍" class="headerlink" title="GAN介绍"></a>GAN介绍</h2><p>GAN$^{[1]}$的本质是最小化生成样本分布和真实数据分布之间的差距。实现思路是交替地训练两个网络$Generator$、$Discriminator$，其中$Gen$要生成尽可能真实的样本，目标是迷惑$Dis$，使得$Dis$难以判别该样本是来自real data还是synthesized data；$Dis$的目标则显然是最大化判别样本真实与否的能力。 </p><h2 id="GAN训练过程"><a href="#GAN训练过程" class="headerlink" title="GAN训练过程"></a>GAN训练过程</h2><p>令$x$表示real data(可以看成一张真实的图片)，$z$表示随机噪声，$D(\cdot)$表示$Discriminator$，$G(\cdot)$表示$Generator$，则$G(z)$即表示生成的假样本synthesized data(可以看成是一张生成的假图)。 </p><p>GAN的目标对$D(\cdot)$而言，是最大化$D(x)$，最小化$D(G(z))$；对$G(\cdot)$而言，要最大化$D(G(z))$。以上的过程可以用一个数学公式统一表达： </p><p>$$arg\underset{G}{Min}\underset{D}{Max}V(G,D)=\mathbb E_{x～P_{real}(x)} \cdot logD(x) + \mathbb E_{z～P_{z}(z)} \cdot log[1-D(G(z))] \tag{1}$$</p><p>其中$\underset{D}{max}V(G,D)$表示$V(G,D)$最大时$D$的取值。<br>训练时首先固定$G$，优化$D$。$V(G,D)$对$D$求导，可得最优的$D$为<br>$$D_G^\ast=\frac{P_{real}(x)}{P_{real}(x)+P_{g}(x)}\tag{2}$$<br>证明:<br>对于连续空间，期望$\mathbb E$的计算方法是求积分，因此有<br>$$\begin{eqnarray}<br>V(G,D)&amp;=&amp;\int_x{P_{real}(x)logD(x){\rm d}x} +\int_z{P_{z}(z)log[1-D(G(z))]{\rm d}z} \tag{3}\\<br>      &amp;=&amp;\int_x{P_{real}(x)logD(x)+P_{g}(x)log[1-D(x)]{\rm d}x} \tag{3})<br>\end{eqnarray}$$</p><p>将其积分符号内的部分对$D$求导，并使其为$0$。<br>$$\frac{dV(G,D)}{dD}=\frac{P_{data}(x)}{D(x)}-\frac{P_g(x)}{1-D(x)}=0\tag{4}$$<br>即得式$(2)$，将其带入式$(3)$得<br>$$\begin{eqnarray}<br>V(G,D^*)&amp;=&amp;-log(4)+KL(P_{data}||\frac{P_{data}+P_{g}}{2})+KL(P_{g}||\frac{P_{data}+P_{g}}{2}) \tag{5}\\<br>&amp;=&amp;-log(4)+2JSD(P_{data}||P_{g}) \tag{5}<br>\end{eqnarray}$$<br>其中$KL(\cdot)$，$JSD(\cdot)$分别表示<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">$KL$散度</a>和<a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">$JS$散度</a>，关于熵跟$KL$散度的关系，也可以点<a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">这里$^{[2]}$</a>。 </p><p>GAN的训练过程可以用下图表示<br>$$\color{red}{\rm{IMAGE}}$$<br>其中黑线表示真实数据分布，绿线表示生成数据分布，蓝线为$Dis$，对于蓝线而言，纵坐标可以表示$Dis$判别为真的概率。图a中是初始化的$Dis$；图b开始训练$Dis$，逐渐能开始识别出real data和synthesized data(分别给出概率为1和0)；图c开始训练$Gen$，生成的样本分布(绿线)开始向真实分布靠拢；图d是最终训练完成的理想状态，生成样本分布和真实样本分布完全重合，$Dis$的判断结果总是0.5，即无法分辨出样本的真假。 </p><h2 id="SeqGAN"><a href="#SeqGAN" class="headerlink" title="SeqGAN"></a>SeqGAN</h2><h3 id="Why-SeqGAN？"><a href="#Why-SeqGAN？" class="headerlink" title="Why SeqGAN？"></a>Why SeqGAN？</h3><p>为什么要引入SeqGAN呢？因为传统的GAN只能解决数据空间连续分布的问题，例如图像的生成问题。图像在计算机中是以二维矩阵的模式存储的，对一张1024*1024的黑白图片而言，就可以用1024*1024个Pixel来表示。如果图片是彩色对，只需再引入一个维度来表示RGB通道。但无论怎样，每个Pixel都是实数空间上可微的，即对某个Pixel进行一个细小的变化${\rm d}Pixel$，在原始图像上就可以表现出该像素点颜色加深/变浅之类的变化。GAN也跟传统的networks一样，使用了back propagation技术，通过对权重矩阵进行一个细微的变化来影响产生图片的质量。由于Pixel连续分布的性质，使得weight matrix的细微变化是有意义的，且能产生作用。 </p><p>而对于离散分布的数据而言，以上的过程就没有意义了。如果我们假设Pixel的分布不再是连续的，而是离散的整数值。比如Pixel=0表示红色，Pixel=1表示黄色，Pixel=2表示蓝色。现在对于图片中的某个像素点而言，它可能原始值是Pixel=0，即表示一个红色的像素点。经过一轮forward propagation和back propagation后，图片的weight matrix发生了一些改变，最终结果是这个Pixel值变成了1.1。问题是，这个1.1表示什么呢？在连续空间上，它可能表示比红色深一点点，或者浅一点点，无论如何，它总是有意义的。但是在离散空间中，它就失去了意义。 </p><p>再来看文本的例子，文字就是典型的离散数据，尽管有word embedding技术，给人感觉上word变成了连续的值，但事实上word在空间上的分布依然是离散的。比如“中国”的embedding是(1.1, 1.2, 1.3, …)，假设它的第一个维度变成了1.2，在word embedding空间上很可能就找不到对应的点了，即使有，它表示的含义跟“中国”也是天差地别，而非之前像素渐变的过程。 </p><p>那如果硬套GAN到文本生成的任务会发生什么呢？首先来看一下整个过程： </p><p>Gen输出softmax之后的结果，即对下一个词在整个词表上的概率预测，sampling一个概率最大的词，重复上述过程直到生成一个句子。Dis对该句子进行判断并优化Gen，优化的方式是通过最小化Dis的loss，然后反传回更新后的参数。问题是梯度在反传时遇到sampling这个环节就中断了（微小的变化并不能影响Sampling的结果）。那有人会说，如果不进行sampling，直接把word distribution传给$Dis$不就好了吗？这样也是不行的，这样对于$Dis$来说，他看到的real data本质上是one hot的形式，而synthesized data则是实数向量的形式，那就$Dis$很容易就判断one hot形式的是真，实数向量形式的是假，而不能真正意义上区分出real data和synthesized data。（如果这里使用了word embedding的技术，可以解决问题。）李宏毅教授在他的课程中也对这个问题作出了一些解释，他的课讲得非常好，可以看<a href="https://www.youtube.com/watch?v=pbQ4qe8EwLo" target="_blank" rel="noopener">这里$^4$</a>。</p><h3 id="SeqGAN是怎么做的？"><a href="#SeqGAN是怎么做的？" class="headerlink" title="SeqGAN是怎么做的？"></a>SeqGAN是怎么做的？</h3><p>跟普通的GAN相比，SeqGAN引入了强化学习的Policy Network。<br>待续</p><h2 id="REFERENCE"><a href="#REFERENCE" class="headerlink" title="REFERENCE"></a>REFERENCE</h2><ol><li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Generative Adversarial Networks</a></li><li><a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">熵和交叉熵</a></li><li><a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener">SeqGAN介绍</a></li><li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" target="_blank" rel="noopener">李宏毅老师的homepage</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SeqGAN </tag>
            
            <tag> GAN </tag>
            
            <tag> RL </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
