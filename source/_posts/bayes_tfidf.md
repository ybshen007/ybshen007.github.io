---
layout:     post
title:      "基于朴素贝叶斯的短文本分类"
date:       2018-10-12
author:     "Shi Lou"
catalog: true
categories: ML
tags:
    - 朴素贝叶斯
    - ML
    - 文本分类
    - 特征工程
---
> 朴素贝叶斯介绍，以及如何将它用在文本分类上。进一步地，如何引入卡方及tf-idf进行特征选择。
<!-- more -->

# 朴素贝叶斯
## 贝叶斯定理简介
关于贝叶斯定理的文章网上已经有很多了，这里只做简单介绍，具体的可以看维基百科上的[定义](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)。

朴素贝叶斯的核心是<font color=red>特征之间是彼此独立的</font>，即 **朴素(naive)** 的含义。尽管事实上特征之间是有关联的，但朴素贝叶斯的效果通常都不错，因此它得以广泛应用。

贝叶斯定理的一般定义：
假设有特征$(f_1, f_2,...,f_n)$以及类别$(C_1, C_2,...,C_m)$。要求该特征下各类别的取值概率$P(C_i|f_1,f_2,...,f_n)$。则有
$$P(C_i|f_1, f_2,...,f_n)=\frac{P(C_i)\cdot P(f_1,f_2,...,f_n|C_i)}{P(f_1,f_2,...,f_n)} \tag{1}$$
其中
$P(C_i)$是先验概率(prior)，可以根据训练集中类目分布统计得出。
$P(C_i|f_1,f_2,...,f_n)$是后验概率(posterior)
$P(f_1,f_2,...,f_n|C_i)$是似然(likehood)
$P(f_1,f_2,...,f_n)$是标准化常量(evidence)

公式(1)在实际应用时会有一个问题，$P(f_1,f_2,...,f_n|C_i)$这一项在计算时，当特征空间增大时，计算量会以指数级增加，造成无法计算的问题。比如每个特征有$k$个取值，则最终特征空间大小是$k^n$，这种计算复杂度是无法接受的。

朴素贝叶斯对此做了一个假设，即特征之间彼此独立。由此可得:
$$P(f_1,f_2,...,f_n|C_i) \approx P(f_1|C_i)P(f_2|C_i)...P(f_n|C_i)$$
这样计算量就缩小到了$kn$量级。

## 朴素贝叶斯分类器
根据朴素贝叶斯假设，贝叶斯定理可以简化成
$$P(C_i|f_1, f_2,...,f_n)=\frac{P(C_i)P(f_1|C_i)P(f_2|C_i)...P(f_n|C_i)}{P(f_1,f_2,...,f_n)} \tag{2}$$
则要求正确的分类
$$C^\ast=\underset{C_i}{Max}P(C_i|f_1,f_2,...,f_n), i=1,...,m$$
对分类器来说，标准化常量项$P(f_1,f_2,...,f_n)$大家都是一样的，因此可以忽略不计，因此最终分类器为:
$$C^\ast=\underset{C_i}{Max}P(C_i)P(f_1|C_i)P(f_2|C_i)...P(f_n|C_i), i=1,...,m \tag{3}$$

# 短文本分类
基于朴素贝叶斯，短文本分类任务可以建模成根据词袋组合求解分类概率的问题。其中先验$P(C_i)$可以根据训练集中已有的类目分布获得。$P(f_j|C_i)$则为类目$C_i$下取得单词$f_j$的概率，可以通过计算指定类目下单词分布获得。
对于NLPCC2017的新闻标题分类[任务](http://tcci.ccf.org.cn/conference/2017/dldoc/taskgline02.pdf)，其数据分布如下:
![数据](/images/data.jpg)
由于只下载了train/dev数据，因此采用train部分数据来构造贝叶斯分类器，dev部分作为测试集。测试集上的部分混淆矩阵如下:
![matrix](/images/matrix.jpg)
可以看到，presicion跟recall分别为0.767和0.741，整体效果还是不错的。

# 特征工程
对于文本分类任务，特征工程通常是指挑选出分类文本中权重更高的词，通过这些词来进行分类。常用的有chi方，互信息法，信息增益法，tf-idf等，这里简单介绍卡方检验和tf-idf方法。

## 卡方检验
### 简介
卡方检验是一种统计量在零假设成立时近似服从卡方分布的假设检验。即认为观测值跟理论值的差异是由随机误差造成的。
对于理论值$E$，有一系列观测值$x_1,x_2,...,x_n$，可得归一化误差
$$\sum_i{\frac{(x_i-E)^2}{E}} \tag{4}$$
当误差大于一定的阈值时，认为原假设不成立。
对于文本分类任务，可以假设<font color=red>词语$w$对分类$C$没有影响</font>。则计算的卡方检验值越大，表明原假设越可能是错误的，即词语$w$对分类$C$有很大的影响。
**注：这里原假设不设置成词语$w$对分类$C$有影响的原因是，无从知晓理论值$E$，即这么假设会造成无法继续计算的问题。**
用一个例子说明，现有二分类任务$C=\{好，坏\}$，样本总数为$N$，对词*笨蛋*计算卡方检验值。

| | 好 | 坏 |
|-|:-:|:-:|
|**含“笨蛋”**|A|B|
|**不含“笨蛋”**|C|D|

对于上述分布，由假设，单词“笨蛋”对类别“好”没有影响。根据大数定理，理论上在总数据集中出现“笨蛋”概率为$\frac {A+B}{N}$，在类目“好”下，产生笨蛋的样本数应该是$(A+C)\frac {A+B}{N}$，即期望$E_{好,笨蛋}$。而事实上出现的数目是$A$。由此可以根据公式(4)计算得$C_{好,笨蛋}$。同理可以计算得到剩下三个值，最终的卡方检验值为:
$$\chi^2(好，笨蛋)=C_{好,笨蛋}+C_{好,不含}+C_{坏,笨蛋}+C_{坏,不含}$$
带入ABCD，上式可化为：
$$\chi^2(word, class)=\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}$$
对所有的词都进行相同操作，逆序排序取topN即前N个对该类目影响最大的词。

### 应用
对上述NLPCC2017的短文本分类任务应用卡方检验选取特征值，对于历史类别下，特征选择结果如下：
![chi](/images/chi.jpg)
可以看到，选择结果还是很不错的。

之后对新闻标题中的词进行筛选，只保留特征选择后的结果进行分类，其中topN中的N分别取50-2000，以50为刻度。presicion变化结果如图：
![presicion](/images/presicion.jpg)
其中最左边是不进行特征选择，可以看到，对于短文本分类任务来说，应用卡方检验进行特征选择的效果并不好。分类器的效果随着N的增大而提高。可能原因是短文本本身信息量就很少，特征稀疏，其每个词已经是一个比较强的特征，进一步筛选后，特征变得更加稀疏，造成了效果下降。

## tf-idf
### 简介
tf-idf是通过统计词在文档中的频率信息来分析词对当前文档的重要程度，和卡方检验一个很大的不同之处在于，tf-idf方法不需要事先知道当前文档的类别。

tf是指词频，即此表中每个词在当前文档中出现的频率。
idf是逆向文件频率，是衡量一个词普遍重要性的指标。假设在所有文档$N$中某个词出现了$n$次，则该词的idf值为$log\frac{N}{n}$，实际计算时为了避免$n$取0，通常对其+1。

tf-idf即两者相乘。用[wiki](https://zh.wikipedia.org/wiki/Tf-idf)上的例子进一步说明。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。
### 应用
还是对前面的任务，进行tf-idf特征选择，各类别top20如下： 
![tfidf](/images/tfidf.jpg)
选择的效果也是非常好的。
### 通过tfidf作为贝叶斯分类的特征
早前贝叶斯分类的权重是通过词袋计算出来的，即指定类别下各单词出现的频率。

对于训练集中的每条样本，可以计算得到该样本中个单词的tfidf值，然后累加到总的类别-tfidf矩阵中的对应位置。矩阵维度为(类目数，词表大小)。

最后对类别-tfidf矩阵进行归一化后，得到贝叶斯分类器的权重矩阵。最后的测试结果准确率为0.7533，和单纯使用bayes效果差不多。


本文涉及的所有代码保存在[Github](https://github.com/ybshen007/classification)上。
